{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3213ded3-cce5-494e-866c-b416be980eaf",
   "metadata": {},
   "source": [
    "# **Dynamo Software (Data Extraction, Formatting, and Reporting assigment)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b48be-001f-42c2-a9f0-17f94223cb2e",
   "metadata": {},
   "source": [
    "### **Setting Up the PostgreSQL Database**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f875aa59-42b2-48ba-b974-69a8daca66a5",
   "metadata": {},
   "source": [
    "To store the extracted and processed financial data, we create a dedicated PostgreSQL database and define a schema that supports both data storage and statistical analysis.\n",
    "\n",
    "#### 1. Create the Database\n",
    "\n",
    "First, create the database named `financial_data`:\n",
    "\n",
    "```sql\n",
    "CREATE DATABASE financial_data;\n",
    "```\n",
    "#### 2. Connect to it \n",
    "``` sql\n",
    "\\c financial_data\r",
    "```\n",
    "#### 3. Create the Main Table: financial_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfc8e7-1b8d-40c2-ac47-e581def742eb",
   "metadata": {},
   "source": [
    "The financial_data table is designed to store each investment record with both required and optional fields. It includes financial metrics, metadata, and indexing for performance:\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS financial_data (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    as_of_date DATE,\n",
    "    original_security_name VARCHAR(255),\n",
    "    investment_in_original DECIMAL(18, 2),\n",
    "    investment_in DECIMAL(18, 2),\n",
    "    investment_in_prior DECIMAL(18, 2),\n",
    "    currency VARCHAR(3),\n",
    "    sector VARCHAR(100),\n",
    "    risk_rating VARCHAR(50),\n",
    "    maturity_date DATE,\n",
    "    yield_percentage DECIMAL(6, 2),\n",
    "    isin VARCHAR(20),\n",
    "    cusip VARCHAR(20),\n",
    "    asset_class VARCHAR(50),\n",
    "    country VARCHAR(100),\n",
    "    region VARCHAR(100),\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "```\n",
    "\n",
    "This table is designed to store structured financial investment data. Below is a column-by-column explanation of what each field does, along with what the data types like `VARCHAR` and `DECIMAL` actually mean.\n",
    "\n",
    "---\n",
    "\n",
    "#### Column-by-Column Breakdown\n",
    "\n",
    "- **`id`** – A unique, auto-incrementing number for each row. `SERIAL` automatically generates values like 1, 2, 3, etc., and acts as the primary key to uniquely identify each record.\n",
    "\n",
    "- **`as_of_date`** – Stores the date when the investment data was recorded. The `DATE` type is used to handle standard calendar dates (e.g., `2024-03-31`).\n",
    "\n",
    "- **`original_security_name`** – Stores the full name of the investment or asset (e.g., \"US Treasury Bond 2026\"). `VARCHAR(255)` means it can hold up to 255 characters of text.\n",
    "\n",
    "- **`investment_in_original`** – The original amount of money that was invested. `DECIMAL(18, 2)` allows up to 18 digits total, with 2 digits after the decimal point (e.g., `1000000.00`), ensuring accurate storage of monetary values.\n",
    "\n",
    "- **`investment_in`** – The current value of the investment. Uses `DECIMAL(18, 2)` for high-precision financial data.\n",
    "\n",
    "- **`investment_in_prior`** – The value of the investment from a previous reporting period. Also uses `DECIMAL(18, 2)`.\n",
    "\n",
    "- **`currency`** – Stores the 3-letter currency code (e.g., USD, EUR). `VARCHAR(3)` allows up to 3 characters.\n",
    "\n",
    "- **`sector`** – Describes the investment's industry sector (e.g., \"Technology\", \"Government\"). `VARCHAR(100)` means it can store up to 100 characters.\n",
    "\n",
    "- **`risk_rating`** – Describes the risk level of the investment (e.g., \"Low\", \"Moderate\", \"High\"). `VARCHAR(50)` allows up to 50 characters.\n",
    "\n",
    "- **`maturity_date`** – Indicates when the investment is expected to mature. Uses the `DATE` type to store standard dates.\n",
    "\n",
    "- **`yield_percentage`** – Represents the investment's annual return rate (e.g., `4.25%`). `DECIMAL(6, 2)` allows up to 6 digits total, including 2 after the decimal point (max value `9999.99`).\n",
    "\n",
    "- **`isin`** - The ISIN (International Securities Identification Number) code for the asset. `VARCHAR(20)` supports standard ISIN formatting.\n",
    "\n",
    "- **`cusip`** - The CUSIP (Committee on Uniform Securities Identification Procedures) code, used for US securities. Stored as `VARCHAR(20)`.\n",
    "\n",
    "- **`asset_class`** - Describes the type of asset (e.g., equity, bond, real estate). `VARCHAR(50)` accommodates common classifications.\n",
    "\n",
    "- **`country`** - Country of risk, origin, or domicile for the asset. Stored as `VARCHAR(100)`.\n",
    "\n",
    "- **`region`** - Geographic or market region (e.g., \"North America\", \"EMEA\"). Also stored as `VARCHAR(100)`.\n",
    "\n",
    "- **`created_at`** – A timestamp showing when the record was first created. `TIMESTAMP DEFAULT CURRENT_TIMESTAMP` automatically stores the current time when a row is inserted.\n",
    "\n",
    "- **`updated_at`** – A timestamp for when the record was last updated. Also uses `TIMESTAMP DEFAULT CURRENT_TIMESTAMP`, but typically updated manually or via a trigger.\n",
    " \n",
    "---\n",
    "\n",
    "This schema ensures precise handling of financial data, proper storage of descriptive fields, and automatic tracking of when records are created and modified.\n",
    "\n",
    "#### Add Indexes for Performance \n",
    "\n",
    "To speed up queries, especially those filtering by date or investment name, create indexes:\n",
    "\n",
    "```sql\n",
    "CREATE INDEX idx_fnancial_data_as_of_date ON financial_data(as_of_date);\n",
    "CREATE INDEX idx_financial_data_security_name ON financial_data(original_security_name);\n",
    "```\n",
    "---\n",
    "\n",
    "#### Create a View for Statistics\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE VIEW financial_data_stats AS\n",
    "SELECT\n",
    "    COUNT(*) AS total_records,\n",
    "\n",
    "    \n",
    "    SUM(CASE WHEN as_of_date IS NOT NULL THEN 1 ELSE 0 END) AS as_of_date_count,\n",
    "    SUM(CASE WHEN original_security_name IS NOT NULL THEN 1 ELSE 0 END) AS original_security_name_count,\n",
    "    SUM(CASE WHEN investment_in_original IS NOT NULL THEN 1 ELSE 0 END) AS investment_in_original_count,\n",
    "    SUM(CASE WHEN investment_in IS NOT NULL THEN 1 ELSE 0 END) AS investment_in_count,\n",
    "    SUM(CASE WHEN investment_in_prior IS NOT NULL THEN 1 ELSE 0 END) AS investment_in_prior_count,\n",
    "    SUM(CASE WHEN currency IS NOT NULL THEN 1 ELSE 0 END) AS currency_count,\n",
    "    COUNT(DISTINCT currency) AS currency_count_distinct,\n",
    "\n",
    "    -- Additional fields(not mandatory can be neglected for this assigment)\n",
    "    SUM(CASE WHEN sector IS NOT NULL THEN 1 ELSE 0 END) AS sector_count,\n",
    "    SUM(CASE WHEN risk_rating IS NOT NULL THEN 1 ELSE 0 END) AS risk_rating_count,\n",
    "    SUM(CASE WHEN maturity_date IS NOT NULL THEN 1 ELSE 0 END) AS maturity_date_count,\n",
    "    SUM(CASE WHEN yield_percentage IS NOT NULL THEN 1 ELSE 0 END) AS yield_percentage_count,\n",
    "    SUM(CASE WHEN isin IS NOT NULL THEN 1 ELSE 0 END) AS isin_count,\n",
    "    SUM(CASE WHEN cusip IS NOT NULL THEN 1 ELSE 0 END) AS cusip_count,\n",
    "    SUM(CASE WHEN asset_class IS NOT NULL THEN 1 ELSE 0 END) AS asset_class_count,\n",
    "    SUM(CASE WHEN country IS NOT NULL THEN 1 ELSE 0 END) AS country_count,\n",
    "    SUM(CASE WHEN region IS NOT NULL THEN 1 ELSE 0 END) AS region_count\n",
    "\n",
    "FROM financial_data;\n",
    "\n",
    "```\n",
    "This view helps verify data completeness and consistency, especially during extraction and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213b27e-06ed-419f-860d-244e56e07c3b",
   "metadata": {},
   "source": [
    "## **Project imports explanation**\n",
    "### Core Data Processing Libraries\n",
    "\n",
    "- re: Regular expressions library for pattern matching. Used to extract field data with different naming patterns.\n",
    "- os: File system operations. Used to determine file extensions for processing different document types.\n",
    "- pandas (as pd): Data manipulation library. Handles data transformation, Excel export, and database storage.\n",
    "- numpy (as np): Numerical computation library. Supports pandas operations and statistical calculations.\n",
    "- datetime: Date and time handling. Used for date validation and formatting operations.\n",
    "\n",
    "### Database Connectivity\n",
    "\n",
    "- psycopg2: PostgreSQL adapter. Provides connection to PostgreSQL databases when selected in configuration.\n",
    "- pymysql: MySQL adapter. Enables connection to MySQL databases when selected in configuration.\n",
    "- sqlalchemy: SQL toolkit and ORM. Provides database-agnostic interface for storing data.\n",
    "- create_engine: SQLAlchemy function. Creates database connection objects for data operations.\n",
    "\n",
    "### Excel File Processing\n",
    "\n",
    "- openpyxl: Excel file library. Handles creation and formatting of output Excel files.\n",
    "- Styling modules (Font, PatternFill, Alignment): Excel formatting tools. Used to enhance readability of output spreadsheets.\n",
    "\n",
    "### Document Processing\n",
    "\n",
    "- docx2txt: Word document processor. Extracts text from .docx files for analysis.\n",
    "- PyPDF2: PDF processor. Extracts textual content from PDF documents.\n",
    "- csv: CSV file handler. Provides direct access to CSV structure and content.\n",
    "- json: JSON processor. Parses JSON files and handles various JSON data structures.\n",
    "\n",
    "### Utilities\n",
    "\n",
    "- logging: Logging framework. Provides structured logging throughout the application.\n",
    "- dateutil.parser (as date_parser): Advanced date parser. Standardizes dates from different formats to MM/DD/YYYY.\n",
    "\n",
    "These libraries collectively form the technological foundation of the financial data extraction system, enabling it to handle various document formats, process structured and unstructured data, and output standardized results to both database and Excel formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2baa25-66e8-4c03-8d9c-d22b63079f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import psycopg2  \n",
    "import pymysql   \n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "import docx2txt  \n",
    "import PyPDF2    \n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "from dateutil import parser as date_parser "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b7196-0895-46f2-a0bf-fdeed1df42e0",
   "metadata": {},
   "source": [
    "## **Configuration and Logging Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2a8eb-996d-44d1-a2b7-cb5233643a46",
   "metadata": {},
   "source": [
    "### Logging Setup\n",
    "\n",
    "This section configures the logging system to capture INFO level messages and above (INFO, WARNING, ERROR, CRITICAL)\n",
    "Sets a consistent format for log messages that includes:\n",
    "- Timestamp (%(asctime)s)\n",
    "- Logger name (%(name)s)\n",
    "- Log level (%(levelname)s)\n",
    "- The actual message (%(message)s)\n",
    "\n",
    "Creates a root logger named 'financial_data_extractor' that will be used throughout the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd4bdeb4-c011-4aa1-8a6c-84ace1e810f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('financial_data_extractor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899b3e3-5147-4b08-82d1-aaf5df3b7853",
   "metadata": {},
   "source": [
    "## **CONFIG Dictionary Explanation**\r\n",
    "\r\n",
    "The `CONFIG` dictionary defines the core parameters for the data extraction pipeline. It allows the script to remain modular and easy to update. The structure includes settings for the database, field extraction rules, and output file details.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 1. `database`\r\n",
    "Specifies details needed to connect to a PostgreSQL database:\r\n",
    "- `type`: Database type (e.g., `\"postgresql\"`)\r\n",
    "- `host`: Server address (e.g., `\"localhost\"`)\r\n",
    "- `port`: Port number for PostgreSQL (usually `5432`)\r\n",
    "- `database`: Name of the database (e.g., `\"financial_data\"`)\r\n",
    "- `user`: Username for authentication\r\n",
    "- `password`: Password for authentication\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. `extraction`\r\n",
    "Defines which fields the script should extract from the document and how to recognize them.\r\n",
    "\r\n",
    "#### `mandatory_fields`\r\n",
    "These are required fields. The pipeline should always try to extract these, and their absence should be logged as missing:\r\n",
    "\r\n",
    "| Field Key               | Purpose                             |\r\n",
    "|-------------------------|-------------------------------------|\r\n",
    "| `as_of_date`            | The report or valuation date        |\r\n",
    "| `original_security_name`| Name of the financial instrument    |\r\n",
    "| `investment_in_original`| Original investment amount          |\r\n",
    "| `investment_in`         | Current investment amount           |\r\n",
    "| `investment_in_prior`   | Prior period investment amount      |\r\n",
    "| `currency`              | Currency type (e.g., USD, EUR)      |\r\n",
    "\r\n",
    "#### `additional_fields`\r\n",
    "These are optional but valuable. If found, they enhance the dataset with more analytics potential:\r\n",
    "\r\n",
    "| Field Key         | Purpose                                       |\r\n",
    "|-------------------|-----------------------------------------------|\r\n",
    "| `sector`          | Economic/industry sector of the investment    |\r\n",
    "| `risk_rating`     | Risk classification of the instrument         |\r\n",
    "| `maturity_date`   | When the instrument matures or expires        |\r\n",
    "| `yield_percentage`| Annualized yield or return rate               |\r\n",
    "| `isin`            | ISIN code (International Securities ID)       |\r\n",
    "| `cusip`           | CUSIP number (US security identifier)         |\r\n",
    "| `asset_class`     | Type of asset (e.g., equity, bond, real estate) |\r\n",
    "| `country`         | Country of risk or domicile                   |\r\n",
    "| `region`          | Geographic or market region                   |\r\n",
    "\r\n",
    "#### `field_variations`\r\n",
    "Each field (mandatory or additional) can appear under different names in various documents. This dictionary maps each field to a list of regular expression (regex) patterns to match its possible labels.\r\n",
    "\r\n",
    "For example:\r\n",
    "- The field `as_of_date` might appear in a document as:\r\n",
    "  - \"As of Date\"\r\n",
    "  - \"Valuation Date\"\r\n",
    "  - \"Statement Date\"\r\n",
    "- The script uses regex patterns like:\r\n",
    "  - `r\"as[\\s_-]*of[\\s_-]*date\"`\r\n",
    "  - `r\"valuation[\\s_-]*date\"`\r\n",
    "\r\n",
    "This design improves extraction accuracy by handling inconsistent document formats.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. `output`\r\n",
    "Specifies output configuration:\r\n",
    "- `excel_file`: The name of the Excel file where extracted and formatted data wived (e.g., `\"extracted_financial_data.xlsx\"`).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Summary\r\n",
    "This configuration enables:\r\n",
    "- Central control of database and output settings.\r\n",
    "- Flexible and accurate field extraction using regex-based field matching.\r\n",
    "- Easy addition of new fields or field name variations without modifying the main extraction logic.\r\n",
    "ds or field name variations without modifying the main extraction logic.\r\n",
    "of new fields or variations without modifying the main extraction logic.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3176705-a359-4f9d-91ba-1b3e4e249c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"database\": {\n",
    "        \"type\": \"postgresql\",  \n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 5432,  \n",
    "        \"database\": \"financial_data\",\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"SenkoSQL\"\n",
    "    },\n",
    "    \"extraction\": {\n",
    "        \"mandatory_fields\": [\n",
    "            \"as_of_date\",\n",
    "            \"original_security_name\",\n",
    "            \"investment_in_original\",\n",
    "            \"investment_in\",\n",
    "            \"investment_in_prior\",\n",
    "            \"currency\"\n",
    "        ],\n",
    "        \"additional_fields\": [\n",
    "            \"sector\",\n",
    "            \"risk_rating\",\n",
    "            \"maturity_date\",\n",
    "            \"yield_percentage\",\n",
    "            \"isin\",\n",
    "            \"cusip\",\n",
    "            \"asset_class\",\n",
    "            \"country\",\n",
    "            \"region\"\n",
    "        ],\n",
    "        # Field name variations for pattern matching\n",
    "        \"field_variations\": {\n",
    "            \"as_of_date\": [\n",
    "                r\"as[\\s_-]*of[\\s_-]*date\",\n",
    "                r\"valuation[\\s_-]*date\", \n",
    "                r\"report[\\s_-]*date\",\n",
    "                r\"date[\\s_-]*of[\\s_-]*valuation\",\n",
    "                r\"statement[\\s_-]*date\"\n",
    "            ],\n",
    "            \"original_security_name\": [\n",
    "                r\"original[\\s_-]*security[\\s_-]*name\",\n",
    "                r\"security[\\s_-]*name\", \n",
    "                r\"instrument[\\s_-]*name\",\n",
    "                r\"asset[\\s_-]*name\",\n",
    "                r\"investment[\\s_-]*name\"\n",
    "            ],\n",
    "            \"investment_in_original\": [\n",
    "                r\"investment[\\s_-]*in[\\s_-]*\\(original\\)\", \n",
    "                r\"original[\\s_-]*investment[\\s_-]*value\",\n",
    "                r\"initial[\\s_-]*investment\",\n",
    "                r\"acquisition[\\s_-]*cost\",\n",
    "                r\"purchase[\\s_-]*value\"\n",
    "            ],\n",
    "            \"investment_in\": [\n",
    "                r\"investment[\\s_-]*in(?!\\s*\\()\", \n",
    "                r\"current[\\s_-]*investment[\\s_-]*value\",\n",
    "                r\"market[\\s_-]*value\",\n",
    "                r\"current[\\s_-]*value\",\n",
    "                r\"present[\\s_-]*value\"\n",
    "            ],\n",
    "            \"investment_in_prior\": [\n",
    "                r\"investment[\\s_-]*in[\\s_-]*\\(prior\\)\", \n",
    "                r\"prior[\\s_-]*investment[\\s_-]*value\",\n",
    "                r\"previous[\\s_-]*value\",\n",
    "                r\"value[\\s_-]*previous[\\s_-]*period\",\n",
    "                r\"last[\\s_-]*period[\\s_-]*value\"\n",
    "            ],\n",
    "            \"currency\": [\n",
    "                r\"currency(?!\\s*type)\", \n",
    "                r\"currency[\\s_-]*type\",\n",
    "                r\"currency[\\s_-]*code\",\n",
    "                r\"denomination\",\n",
    "                r\"traded[\\s_-]*in\"\n",
    "            ],\n",
    "            \"sector\": [\n",
    "                r\"sector\",\n",
    "                r\"industry[\\s_-]*sector\",\n",
    "                r\"business[\\s_-]*sector\",\n",
    "                r\"market[\\s_-]*sector\"\n",
    "            ],\n",
    "            \"risk_rating\": [\n",
    "                r\"risk[\\s_-]*rating\",\n",
    "                r\"risk[\\s_-]*level\",\n",
    "                r\"risk[\\s_-]*assessment\",\n",
    "                r\"risk[\\s_-]*profile\"\n",
    "            ],\n",
    "            \"maturity_date\": [\n",
    "                r\"maturity[\\s_-]*date\",\n",
    "                r\"expiry[\\s_-]*date\",\n",
    "                r\"expiration[\\s_-]*date\",\n",
    "                r\"term[\\s_-]*end[\\s_-]*date\"\n",
    "            ],\n",
    "            \"yield_percentage\": [\n",
    "                r\"yield[\\s_-]*percentage\",\n",
    "                r\"yield[\\s_-]*rate\",\n",
    "                r\"yield[\\s_-]*\\%\",\n",
    "                r\"annual[\\s_-]*yield\",\n",
    "                r\"rate[\\s_-]*of[\\s_-]*return\"\n",
    "            ],\n",
    "            \"isin\": [\n",
    "                r\"isin\",\n",
    "                r\"international[\\s_-]*securities[\\s_-]*identification[\\s_-]*number\"\n",
    "            ],\n",
    "            \"cusip\": [\n",
    "                r\"cusip\",\n",
    "                r\"committee[\\s_-]*on[\\s_-]*uniform[\\s_-]*securities[\\s_-]*identification[\\s_-]*procedures\"\n",
    "            ],\n",
    "            \"asset_class\": [\n",
    "                r\"asset[\\s_-]*class\",\n",
    "                r\"asset[\\s_-]*type\",\n",
    "                r\"investment[\\s_-]*type\",\n",
    "                r\"instrument[\\s_-]*class\"\n",
    "            ],\n",
    "            \"country\": [\n",
    "                r\"country\",\n",
    "                r\"country[\\s_-]*of[\\s_-]*risk\",\n",
    "                r\"country[\\s_-]*of[\\s_-]*domicile\",\n",
    "                r\"domicile\"\n",
    "            ],\n",
    "            \"region\": [\n",
    "                r\"region\",\n",
    "                r\"geographic[\\s_-]*region\",\n",
    "                r\"market[\\s_-]*region\"\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"excel_file\": \"extracted_financial_data.xlsx\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9953887-3c5c-4d48-9940-43eae43606cc",
   "metadata": {},
   "source": [
    "# **DocumentExtractor Class - Explanation**\r\n",
    "\r\n",
    "The `DocumentExtractor` class is responsible for loading documents of various types (Word, PDF, text, CSV, JSON), extracting financial fields using pattern matching, and returning clean structured data records.\r\n",
    "\r\n",
    "## Key Components:\r\n",
    "\r\n",
    "### 1. Initialization (`__init__`)\r\n",
    "- Initializes the class with a file path.\r\n",
    "- Identifies the file type based on extension.\r\n",
    "- Sets up placeholders for raw text and extracted data.\r\n",
    "- Configures a logger for debugging and tracking.\r\n",
    "\r\n",
    "### 2. `extract_text()`\r\n",
    "- Extracts text from supported file types:\r\n",
    "  - `.docx` via `docx2txt`\r\n",
    "  - `.pdf` via `PyPDF2`\r\n",
    "  - `.txt` via standard reading\r\n",
    "  - `.csv` as rows converted to plain text\r\n",
    "  - `.json` converted to indented string format\r\n",
    "- Logs success or raises errors on failure.\r\n",
    "\r\n",
    "### 3. `_build_field_pattern()`\r\n",
    "- Builds regex patterns for each field based on variations listed in the `CONFIG`.\r\n",
    "- Helps match different label formats across documents.\r\n",
    "\r\n",
    "### 4. `_extract_date_global()`\r\n",
    "- Searches the document for common date patterns labeled as \"As of Date\", \"Valuation Date\", etc.\r\n",
    "- Returns the first date found.\r\n",
    "\r\n",
    "### 5. `_identify_investment_sections()`\r\n",
    "- Splits the document into sections that likely correspond to different investments.\r\n",
    "- Uses multiple regex patterns to detect headings or delimiters.\r\n",
    "\r\n",
    "### 6. `_extract_table_data()`\r\n",
    "- Looks for tabular data based on headers and formatting.\r\n",
    "- Parses rows and maps each column to standard fields like `security_name`, `currency`, etc.\r\n",
    "\r\n",
    "### 7. `_clean_field_value()`\r\n",
    "- Standardizes field values:\r\n",
    "  - Strips extra characters\r\n",
    "  - Extracts numbers from mixed content\r\n",
    "  - Normalizes currency codes\r\n",
    "\r\n",
    "### 8. `extract_data()`\r\n",
    "- Orchestrates the full data extraction process:\r\n",
    "  1. Ensures text is loaded.\r\n",
    "  2. Extracts global `as_of_date`.\r\n",
    "  3. Tries three strategies:\r\n",
    "     - Table-based extraction\r\n",
    "     - Section-based extraction\r\n",
    "     - Global regex scanning\r\n",
    "  4. Applies field name mapping and value cleaning.\r\n",
    "  5. Supports special handling for `.csv` and `.json` formats.\r\n",
    "  6. Validates records (must have required fields).\r\n",
    "\r\n",
    "- Returns a list of structured data entries suitable for export to Excel or a database.\r\n",
    "\r\n",
    "## Summary\r\n",
    "The `DocumentExtractor` class is robust and flexible. It ensures consistent, accurate extraction of key investment data across various unstructured document types using multiple strategies, pattern matching, and data cleaning techniques.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae6ebaa-6934-480e-ab3d-540ee8be47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentExtractor:\n",
    "    \"\"\"Handle extraction of data from various document types with robust pattern matching\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        self.raw_text = \"\"\n",
    "        self.extracted_data = []\n",
    "        self.logger = logging.getLogger('financial_data_extractor.DocumentExtractor')\n",
    "        \n",
    "    def extract_text(self):\n",
    "        \"\"\"Extract raw text from document based on file extension\"\"\"\n",
    "        self.logger.info(f\"Extracting text from {self.file_path} (type: {self.file_extension})\")\n",
    "        \n",
    "        try:\n",
    "            if self.file_extension == \".docx\":\n",
    "                self.raw_text = docx2txt.process(self.file_path)\n",
    "            elif self.file_extension == \".pdf\":\n",
    "                with open(self.file_path, \"rb\") as file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(file)\n",
    "                    for page_num in range(len(pdf_reader.pages)):\n",
    "                        page = pdf_reader.pages[page_num]\n",
    "                        self.raw_text += page.extract_text()\n",
    "            elif self.file_extension == \".txt\":\n",
    "                with open(self.file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as file:\n",
    "                    self.raw_text = file.read()\n",
    "            elif self.file_extension == \".csv\":\n",
    "                with open(self.file_path, 'r', encoding='utf-8', errors=\"replace\") as file:\n",
    "                    reader = csv.reader(file)\n",
    "                    for row in reader:\n",
    "                        self.raw_text += ' '.join(row) + '\\n'\n",
    "            elif self.file_extension == \".json\":\n",
    "                with open(self.file_path, 'r', encoding='utf-8', errors=\"replace\") as file:\n",
    "                    data = json.load(file)\n",
    "                    # This is used to convert JSON to text representation\n",
    "                    self.raw_text = json.dumps(data, indent=2)\n",
    "            else:\n",
    "                self.logger.error(f\"Unsupported file format: {self.file_extension}\")\n",
    "                raise ValueError(f\"Unsupported file format: {self.file_extension}\")\n",
    "            \n",
    "            self.logger.info(f\"Successfully extracted {len(self.raw_text)} characters of text\")\n",
    "            return self.raw_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting text from file: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _build_field_pattern(self, field_name):\n",
    "        \"\"\"Build a comprehensive regex pattern for a field based on all its variations\"\"\"\n",
    "        variations = CONFIG[\"extraction\"][\"field_variations\"].get(field_name, [field_name.lower()])\n",
    "        pattern_parts = []\n",
    "        \n",
    "        for variation in variations:\n",
    "            # Match the field name followed by: colon, equals, dash, or space then colon\n",
    "            pattern_parts.append(f\"(?:{variation})\\\\s*[:=\\\\-]\\\\s*([^\\\\n:]+)\")\n",
    "        \n",
    "        # Join all variations with OR (|)\n",
    "        return '|'.join(pattern_parts)\n",
    "    \n",
    "    def _extract_date_global(self):\n",
    "        \"\"\"Extract a global as_of_date from the document\"\"\"\n",
    "        date_patterns = [\n",
    "            r\"(?:As of|Valuation|Report|Statement)(?:\\s+[Dd]ate)?:\\s+([0-9]{1,2}[\\\\/\\\\-\\\\.][0-9]{1,2}[\\\\/\\\\-\\\\.][0-9]{2,4})\",\n",
    "            r\"(?:As of|Valuation|Report|Statement)(?:\\s+[Dd]ate)?:\\s+([A-Za-z]+\\s+[0-9]{1,2},?\\s+[0-9]{2,4})\",\n",
    "            r\"[Dd]ate:\\s+([0-9]{1,2}[\\\\/\\\\-\\\\.][0-9]{1,2}[\\\\/\\\\-\\\\.][0-9]{2,4})\",\n",
    "            r\"[Dd]ate:\\s+([A-Za-z]+\\s+[0-9]{1,2},?\\s+[0-9]{2,4})\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in date_patterns:\n",
    "            date_match = re.search(pattern, self.raw_text, re.IGNORECASE)\n",
    "            if date_match:\n",
    "                return date_match.group(1).strip()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _identify_investment_sections(self):\n",
    "        \"\"\"Identify individual investment sections in the document using multiple patterns\"\"\"\n",
    "        section_patterns = [\n",
    "            # Numbered investment sections\n",
    "            r\"(?:#{1,4}|Section|SECTION)\\s+(?:INVESTMENT|Investment|Asset|ASSET)\\s+(?:\\d+|\\w+)(?:\\s*:)?\\s*\\n(.*?)(?=(?:#{1,4}|Section|SECTION)\\s+(?:INVESTMENT|Investment|Asset|ASSET)\\s+(?:\\d+|\\w+)|$)\",\n",
    "            \n",
    "            # Sections with clear dividers\n",
    "            r\"(?:[-=*]{3,})\\n\\s*(?:INVESTMENT|Investment|Asset|ASSET)\\s+(?:\\d+|\\w+)\\s*\\n(?:[-=*]{3,})\\n(.*?)(?=(?:[-=*]{3,})\\n\\s*(?:INVESTMENT|Investment|Asset|ASSET)\\s+(?:\\d+|\\w+)|$)\",\n",
    "            \n",
    "            # Sections with empty line dividers\n",
    "            r\"\\n\\s*(?:INVESTMENT|Investment|Asset|ASSET)\\s+(?:\\d+|\\w+)\\s*\\n\\n(.*?)(?=\\n\\n\\s*(?:INVESTMENT|Investment|Asset|ASSET)\\s+(?:\\d+|\\w+)|$)\",\n",
    "            \n",
    "            # Named investment sections\n",
    "            r\"(?:Investment|INVESTMENT|Asset|ASSET)(?:\\s+in|\\s+name)?:\\s+([^\\n]+)\\n(.*?)(?=(?:Investment|INVESTMENT|Asset|ASSET)(?:\\s+in|\\s+name)?:\\s+|$)\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in section_patterns:\n",
    "            sections = re.findall(pattern, self.raw_text, re.DOTALL | re.IGNORECASE)\n",
    "            if sections:\n",
    "                # If found sections using this pattern, return them\n",
    "                if isinstance(sections[0], tuple):\n",
    "                    # If pattern captured multiple groups, use the last one\n",
    "                    return [s[-1] for s in sections]\n",
    "                return sections\n",
    "        \n",
    "        # If no sections found, return the whole document as one section\n",
    "        return [self.raw_text]\n",
    "    \n",
    "    def _extract_table_data(self):\n",
    "        \"\"\"Try to extract data from tabular formats in the document\"\"\"\n",
    "        # Look for tabular data patterns - simple CSV-like or fixed width formats\n",
    "        table_patterns = [\n",
    "            # Header row followed by data rows\n",
    "            r\"(?:Security\\s+Name|Asset\\s+Name|Investment)[\\s,|]+(?:Original|Initial)[\\s,|]+(?:Current|Market)[\\s,|]+(?:Prior|Previous)[\\s,|]+Currency\\s*\\n((?:.*\\n)+)\"\n",
    "        ]\n",
    "        \n",
    "        table_data = []\n",
    "        \n",
    "        for pattern in table_patterns:\n",
    "            matches = re.search(pattern, self.raw_text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                table_content = matches.group(1)\n",
    "                rows = table_content.strip().split('\\n')\n",
    "                \n",
    "                for row in rows:\n",
    "                    # Split by common delimiters\n",
    "                    columns = re.split(r'\\s{2,}|,|\\||\\t', row.strip())\n",
    "                    if len(columns) >= 5:  # Expecting at least 5 columns for basic data(exchange to whatever needed)\n",
    "                        data_dict = {}\n",
    "                        \n",
    "                        # Map columns to fields based on position\n",
    "                        # This is a simple heuristic and might need adjustment\n",
    "                        if len(columns) >= 1:\n",
    "                            data_dict[\"original_security_name\"] = columns[0].strip()\n",
    "                        if len(columns) >= 2:\n",
    "                            data_dict[\"investment_in_original\"] = columns[1].strip()\n",
    "                        if len(columns) >= 3:\n",
    "                            data_dict[\"investment_in\"] = columns[2].strip()\n",
    "                        if len(columns) >= 4:\n",
    "                            data_dict[\"investment_in_prior\"] = columns[3].strip()\n",
    "                        if len(columns) >= 5:\n",
    "                            data_dict[\"currency\"] = columns[4].strip()\n",
    "                        \n",
    "                        table_data.append(data_dict)\n",
    "        \n",
    "        return table_data\n",
    "    \n",
    "    def _clean_field_value(self, field, value):\n",
    "        \"\"\"Clean and standardize field values\"\"\"\n",
    "        if not value:\n",
    "            return value\n",
    "            \n",
    "        value = value.strip()\n",
    "        \n",
    "        # Standard field-specific cleaning\n",
    "        if field in [\"investment_in_original\", \"investment_in\", \"investment_in_prior\"]:\n",
    "            # Extract numeric part\n",
    "            matches = re.search(r'[-+]?[0-9,.]+(?:\\.[0-9]+)?', value)\n",
    "            if matches:\n",
    "                return matches.group(0).replace(',', '')\n",
    "                \n",
    "        elif field == \"currency\":\n",
    "            # Extract currency code\n",
    "            matches = re.search(r'\\b(USD|EUR|GBP|JPY|CHF|CAD|AUD|NZD|[A-Z]{3})\\b', value.upper())\n",
    "            if matches:\n",
    "                return matches.group(1)\n",
    "            return value.upper()\n",
    "            \n",
    "        elif field == \"yield_percentage\":\n",
    "            # Extract percentage value\n",
    "            matches = re.search(r'[-+]?[0-9,.]+(?:\\.[0-9]+)?', value)\n",
    "            if matches:\n",
    "                return matches.group(0).replace(',', '')\n",
    "                \n",
    "        return value\n",
    "    \n",
    "    def extract_data(self):\n",
    "        \"\"\"Extract structured data from the text using multiple strategies\"\"\"\n",
    "        # Make sure we have text to process\n",
    "        if not self.raw_text:\n",
    "            self.extract_text()\n",
    "        \n",
    "        self.logger.info(\"Starting data extraction\")\n",
    "        \n",
    "        # Initialize data dictionaries list\n",
    "        data_dicts = []\n",
    "        \n",
    "        # Global date - looks for the as_of_date field in the whole document\n",
    "        as_of_date = self._extract_date_global()\n",
    "        self.logger.info(f\"Global as_of_date extracted: {as_of_date}\")\n",
    "        \n",
    "        # First attempt: Try to extract data from tabular format\n",
    "        table_data = self._extract_table_data()\n",
    "        if table_data:\n",
    "            self.logger.info(f\"Extracted {len(table_data)} records from tabular format\")\n",
    "            \n",
    "            # Add the global date to each record\n",
    "            for record in table_data:\n",
    "                if as_of_date:\n",
    "                    record[\"as_of_date\"] = as_of_date\n",
    "            \n",
    "            data_dicts.extend(table_data)\n",
    "        \n",
    "        # Second attempt: Find investment sections and extract from each\n",
    "        if not data_dicts:\n",
    "            self.logger.info(\"No tabular data found, trying section-based extraction\")\n",
    "            investment_sections = self._identify_investment_sections()\n",
    "            self.logger.info(f\"Identified {len(investment_sections)} investment sections\")\n",
    "            \n",
    "            # Build patterns dictionary for all fields\n",
    "            field_patterns = {}\n",
    "            for field_name in CONFIG[\"extraction\"][\"mandatory_fields\"] + CONFIG[\"extraction\"][\"additional_fields\"]:\n",
    "                field_patterns[field_name] = self._build_field_pattern(field_name)\n",
    "            \n",
    "            # Process each investment section\n",
    "            for section in investment_sections:\n",
    "                data_dict = {}\n",
    "                \n",
    "                # Add the global date to each investment record\n",
    "                if as_of_date:\n",
    "                    data_dict[\"as_of_date\"] = as_of_date\n",
    "                \n",
    "                # Extract each field from the section\n",
    "                for field, pattern in field_patterns.items():\n",
    "                    match = re.search(pattern, section, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        # Use the last group in case there are multiple capture groups\n",
    "                        data_dict[field] = match.group(match.lastindex or 1).strip()\n",
    "                \n",
    "                # Map field variations to standardized field names\n",
    "                # This is for fields that might be extracted with different names\n",
    "                field_mapping = {\n",
    "                    \"security_name\": \"original_security_name\",\n",
    "                    \"instrument_name\": \"original_security_name\",\n",
    "                    \"asset_name\": \"original_security_name\",\n",
    "                    \n",
    "                    \"original_investment_value\": \"investment_in_original\",\n",
    "                    \"initial_investment\": \"investment_in_original\",\n",
    "                    \"acquisition_cost\": \"investment_in_original\",\n",
    "                    \"purchase_value\": \"investment_in_original\",\n",
    "                    \n",
    "                    \"current_investment_value\": \"investment_in\",\n",
    "                    \"market_value\": \"investment_in\",\n",
    "                    \"current_value\": \"investment_in\",\n",
    "                    \"present_value\": \"investment_in\",\n",
    "                    \n",
    "                    \"prior_investment_value\": \"investment_in_prior\",\n",
    "                    \"previous_value\": \"investment_in_prior\",\n",
    "                    \"value_previous_period\": \"investment_in_prior\",\n",
    "                    \"last_period_value\": \"investment_in_prior\",\n",
    "                    \n",
    "                    \"currency_type\": \"currency\",\n",
    "                    \"currency_code\": \"currency\",\n",
    "                    \"denomination\": \"currency\",\n",
    "                    \n",
    "                    \"risk_level\": \"risk_rating\",\n",
    "                    \"risk_assessment\": \"risk_rating\",\n",
    "                    \"risk_profile\": \"risk_rating\",\n",
    "                    \n",
    "                    \"expiry_date\": \"maturity_date\",\n",
    "                    \"expiration_date\": \"maturity_date\",\n",
    "                    \"term_end_date\": \"maturity_date\",\n",
    "                    \n",
    "                    \"yield_rate\": \"yield_percentage\",\n",
    "                    \"yield\": \"yield_percentage\",\n",
    "                    \"annual_yield\": \"yield_percentage\",\n",
    "                    \"rate_of_return\": \"yield_percentage\"\n",
    "                }\n",
    "                \n",
    "                # Standardize field names\n",
    "                for old_field, new_field in field_mapping.items():\n",
    "                    if old_field in data_dict and old_field != new_field:\n",
    "                        if new_field not in data_dict or not data_dict[new_field]:\n",
    "                            data_dict[new_field] = data_dict[old_field]\n",
    "                        del data_dict[old_field]\n",
    "                \n",
    "                # Clean and standardize field values\n",
    "                for field, value in list(data_dict.items()):\n",
    "                    if value:\n",
    "                        data_dict[field] = self._clean_field_value(field, value)\n",
    "                \n",
    "                # Only add if we found at least one field\n",
    "                if len(data_dict) > 1 or (len(data_dict) == 1 and \"as_of_date\" not in data_dict):\n",
    "                    data_dicts.append(data_dict)\n",
    "        \n",
    "        # Third attempt: If no investment sections were found, try a more general approach\n",
    "        if not data_dicts:\n",
    "            self.logger.info(\"No investment sections found, trying general extraction\")\n",
    "            \n",
    "            # Create a record for the as_of_date if found\n",
    "            if as_of_date:\n",
    "                data_dicts.append({\"as_of_date\": as_of_date})\n",
    "            \n",
    "            # Build patterns dictionary\n",
    "            field_patterns = {}\n",
    "            for field_name in CONFIG[\"extraction\"][\"mandatory_fields\"] + CONFIG[\"extraction\"][\"additional_fields\"]:\n",
    "                field_patterns[field_name] = self._build_field_pattern(field_name)\n",
    "            \n",
    "            # Try to find individual fields across the whole document\n",
    "            for field, pattern in field_patterns.items():\n",
    "                matches = re.findall(pattern, self.raw_text, re.IGNORECASE)\n",
    "                for i, match in enumerate(matches):\n",
    "                    # Create new dictionaries as needed\n",
    "                    while i >= len(data_dicts):\n",
    "                        data_dicts.append({})\n",
    "                    \n",
    "                    # Handle tuple results from multiple capture groups\n",
    "                    if isinstance(match, tuple):\n",
    "                        # Find the first non-empty group\n",
    "                        for group in match:\n",
    "                            if group.strip():\n",
    "                                data_dict[field] = self._clean_field_value(field, group)\n",
    "                                break\n",
    "                    else:\n",
    "                        data_dicts[i][field] = self._clean_field_value(field, match)\n",
    "        \n",
    "        # Special handling for CSV or JSON files\n",
    "        if self.file_extension == \".csv\":\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='utf-8', errors=\"replace\") as file:\n",
    "                    reader = csv.DictReader(file)\n",
    "                    rows = list(reader)\n",
    "                \n",
    "                if rows:\n",
    "                    self.logger.info(f\"Extracted {len(rows)} records from CSV file\")\n",
    "                    \n",
    "                    # Map CSV headers to our field names\n",
    "                    field_mapping = {\n",
    "                        \"Security Name\": \"original_security_name\",\n",
    "                        \"Asset Name\": \"original_security_name\",\n",
    "                        \"Instrument Name\": \"original_security_name\",\n",
    "                        \"Security\": \"original_security_name\",\n",
    "                        \n",
    "                        \"Initial Investment\": \"investment_in_original\",\n",
    "                        \"Original Investment\": \"investment_in_original\",\n",
    "                        \"Acquisition Cost\": \"investment_in_original\",\n",
    "                        \"Purchase Value\": \"investment_in_original\",\n",
    "                        \n",
    "                        \"Market Value\": \"investment_in\",\n",
    "                        \"Current Value\": \"investment_in\",\n",
    "                        \"Present Value\": \"investment_in\",\n",
    "                        \"Current Investment\": \"investment_in\",\n",
    "                        \n",
    "                        \"Prior Value\": \"investment_in_prior\",\n",
    "                        \"Previous Value\": \"investment_in_prior\",\n",
    "                        \"Last Period Value\": \"investment_in_prior\",\n",
    "                        \"Prior Investment\": \"investment_in_prior\",\n",
    "                        \n",
    "                        \"Currency Type\": \"currency\",\n",
    "                        \"Currency Code\": \"currency\",\n",
    "                        \"Denomination\": \"currency\",\n",
    "                        \n",
    "                        \"Risk Level\": \"risk_rating\",\n",
    "                        \"Risk Assessment\": \"risk_rating\",\n",
    "                        \"Risk Profile\": \"risk_rating\",\n",
    "                        \n",
    "                        \"Expiry Date\": \"maturity_date\",\n",
    "                        \"Expiration Date\": \"maturity_date\",\n",
    "                        \"Term End Date\": \"maturity_date\",\n",
    "                        \n",
    "                        \"Yield Rate\": \"yield_percentage\",\n",
    "                        \"Yield\": \"yield_percentage\",\n",
    "                        \"Annual Yield\": \"yield_percentage\",\n",
    "                        \"Rate of Return\": \"yield_percentage\",\n",
    "                        \"Yield %\": \"yield_percentage\"\n",
    "                    }\n",
    "                    \n",
    "                    csv_data = []\n",
    "                    for row in rows:\n",
    "                        data_dict = {}\n",
    "                        \n",
    "                        # Add the global date if found\n",
    "                        if as_of_date:\n",
    "                            data_dict[\"as_of_date\"] = as_of_date\n",
    "                        \n",
    "                        # Map fields\n",
    "                        for csv_field, value in row.items():\n",
    "                            # Try to map the field to our standard field names\n",
    "                            target_field = field_mapping.get(csv_field, csv_field.lower().replace(' ', '_'))\n",
    "                            \n",
    "                            # Clean and standardize the value\n",
    "                            if value:\n",
    "                                data_dict[target_field] = self._clean_field_value(target_field, value)\n",
    "                        \n",
    "                        if len(data_dict) > 1 or (len(data_dict) == 1 and \"as_of_date\" not in data_dict):\n",
    "                            csv_data.append(data_dict)\n",
    "                    \n",
    "                    # If we found data in the CSV, use that instead\n",
    "                    if csv_data:\n",
    "                        data_dicts = csv_data\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error processing CSV file: {str(e)}\")\n",
    "        \n",
    "        elif self.file_extension == \".json\":\n",
    "            try:\n",
    "                with open(self.file_path, 'r', encoding='utf-8', errors=\"replace\") as file:\n",
    "                    json_data = json.load(file)\n",
    "                \n",
    "                # Try to extract structured data from JSON\n",
    "                json_records = []\n",
    "                \n",
    "                # Look for common JSON structures\n",
    "                if isinstance(json_data, dict):\n",
    "                    # Case 1: {\"report_date\": \"...\", \"investments\": [{...}, {...}]}\n",
    "                    if \"investments\" in json_data and isinstance(json_data[\"investments\"], list):\n",
    "                        report_date = json_data.get(\"report_date\") or json_data.get(\"as_of_date\") or as_of_date\n",
    "                        \n",
    "                        for investment in json_data[\"investments\"]:\n",
    "                            data_dict = {\"as_of_date\": report_date} if report_date else {}\n",
    "                            \n",
    "                            # Map JSON fields to our standard fields\n",
    "                            field_mapping = {\n",
    "                                \"security_name\": \"original_security_name\",\n",
    "                                \"instrument_name\": \"original_security_name\",\n",
    "                                \"asset_name\": \"original_security_name\",\n",
    "                                \n",
    "                                \"investment_original\": \"investment_in_original\",\n",
    "                                \"original_investment\": \"investment_in_original\",\n",
    "                                \"investment_original\": \"investment_in_original\",\n",
    "                                \"original_investment\": \"investment_in_original\",\n",
    "                                \"initial_investment\": \"investment_in_original\",\n",
    "                                \"acquisition_cost\": \"investment_in_original\",\n",
    "                                \n",
    "                                \"investment_current\": \"investment_in\",\n",
    "                                \"current_investment\": \"investment_in\",\n",
    "                                \"market_value\": \"investment_in\",\n",
    "                                \"current_value\": \"investment_in\",\n",
    "                                \n",
    "                                \"investment_prior\": \"investment_in_prior\",\n",
    "                                \"prior_investment\": \"investment_in_prior\",\n",
    "                                \"previous_value\": \"investment_in_prior\",\n",
    "                                \"last_period_value\": \"investment_in_prior\"\n",
    "                            }\n",
    "                            \n",
    "                            for json_field, value in investment.items():\n",
    "                                # Map to standard field name if needed\n",
    "                                field_name = field_mapping.get(json_field, json_field)\n",
    "                                \n",
    "                                # Clean and standardize the value\n",
    "                                if value is not None:\n",
    "                                    data_dict[field_name] = self._clean_field_value(field_name, str(value))\n",
    "                            \n",
    "                            if data_dict:\n",
    "                                json_records.append(data_dict)\n",
    "                    \n",
    "                    # Case 2: {\"investments\": {\"investment1\": {...}, \"investment2\": {...}}}\n",
    "                    elif \"investments\" in json_data and isinstance(json_data[\"investments\"], dict):\n",
    "                        report_date = json_data.get(\"report_date\") or json_data.get(\"as_of_date\") or as_of_date\n",
    "                        \n",
    "                        for investment_name, investment_data in json_data[\"investments\"].items():\n",
    "                            data_dict = {\"as_of_date\": report_date} if report_date else {}\n",
    "                            data_dict[\"original_security_name\"] = investment_name\n",
    "                            \n",
    "                            for json_field, value in investment_data.items():\n",
    "                                if value is not None:\n",
    "                                    data_dict[json_field] = self._clean_field_value(json_field, str(value))\n",
    "                            \n",
    "                            if data_dict:\n",
    "                                json_records.append(data_dict)\n",
    "                \n",
    "                # Case 3: Direct list of investments\n",
    "                elif isinstance(json_data, list):\n",
    "                    for investment in json_data:\n",
    "                        if isinstance(investment, dict):\n",
    "                            data_dict = {}\n",
    "                            \n",
    "                            # Add the global date if found\n",
    "                            if as_of_date:\n",
    "                                data_dict[\"as_of_date\"] = as_of_date\n",
    "                            \n",
    "                            for json_field, value in investment.items():\n",
    "                                if value is not None:\n",
    "                                    data_dict[json_field] = self._clean_field_value(json_field, str(value))\n",
    "                            \n",
    "                            if data_dict:\n",
    "                                json_records.append(data_dict)\n",
    "                \n",
    "                # If we found records in the JSON, use them\n",
    "                if json_records:\n",
    "                    self.logger.info(f\"Extracted {len(json_records)} records from JSON file\")\n",
    "                    data_dicts = json_records\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error processing JSON file: {str(e)}\")\n",
    "        \n",
    "        # Final validation and cleanup\n",
    "        validated_data = []\n",
    "        for record in data_dicts:\n",
    "            # Only include records with at least some mandatory fields\n",
    "            mandatory_fields_count = sum(1 for field in CONFIG[\"extraction\"][\"mandatory_fields\"] if field in record and record[field])\n",
    "            \n",
    "            # Keep if we have as_of_date and at least one other mandatory field, or at least 2 mandatory fields\n",
    "            if (mandatory_fields_count >= 2) or (\"as_of_date\" in record and mandatory_fields_count >= 1):\n",
    "                validated_data.append(record)\n",
    "        \n",
    "        self.extracted_data = validated_data\n",
    "        self.logger.info(f\"Extracted {len(validated_data)} valid records with data\")\n",
    "        return validated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbede4-d3b8-4355-8629-85aba6b8345b",
   "metadata": {},
   "source": [
    "# **DataProcessor Class - Explanation**\r\n",
    "\r\n",
    "The `DataProcessor` class takes in raw extracted data (usually from the `DocumentExtractor`) and:\r\n",
    "- Formats dates, currencies, and percentages consistently\r\n",
    "- Calculates summary statistics about extraction quality\r\n",
    "- Flags missing or inconsistent values\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Key Methods\r\n",
    "\r\n",
    "### 1. `__init__(self, data_list)`\r\n",
    "Initializes the processor with a list of extracted records (`data_list`).\r\n",
    "- `self.raw_data`: The unprocessed records\r\n",
    "- `self.processed_data`: Will store cleaned, formatted results\r\n",
    "- `self.extraction_stats`: Will store metadata and summary info\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. `_format_date(date_str)`\r\n",
    "Attempts to convert various date formats into a standardized `MM/DD/YYYY` format using `dateutil.parser`. If parsing fails, the original string is returned.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. `_format_currency(value_str)`\r\n",
    "Converts numeric-like strings into a standardized 2-decimal format, e.g., `1000` → `1000.00`. Removes extra characters like commas and symbols.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. `_format_percentage(value_str)`\r\n",
    "Same logic as `_format_currency()`, but intended for percentages. Removes non-numeric characters and formats to two decimal places.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 5. `format_data()`\r\n",
    "Loops through each record and applies the formatting functions:\r\n",
    "- Dates → `_format_date()`\r\n",
    "- Currency fields → `_format_currency()`\r\n",
    "- Percentages → `_format_percentage()`\r\n",
    "\r\n",
    "Also copies over all other fields as-is.\r\n",
    "Returns a list of formatted records.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 6. `calculate_statistics()`\r\n",
    "Analyzes the formatted data and generates metadata:\r\n",
    "- How many records were processed\r\n",
    "- How many mandatory fields were filled in per record\r\n",
    "- What fields are missing (and how often)\r\n",
    "- Checks for inconsistent values:\r\n",
    "  - Multiple currencies\r\n",
    "  - Inconsistent date formats\r\n",
    "- Calculates overall \"extraction accuracy\" with weights:\r\n",
    "  - 70% importance to mandatory fields\r\n",
    "  - 30% to additional fields\r\n",
    "\r\n",
    "Stores and returns all stats as a dictionary.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 7. `_identify_date_format(date_str)`\r\n",
    "Simple regex-based method to infer a date's format (e.g., `MM/DD/YYYY`, `YYYY-MM-DD`, etc.). Used for consistency checks.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Summary\r\n",
    "The `DataProcessor` class is essential for transforming raw, inconsistently formatted data into clean, validated records. It also provides useful metrics for evaluating the quality of the extraction process and identifying potential data issues.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8d0e5df-f077-4138-9449-fa6ac9de6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Process and format the extracted data with robust type handling and conversions\"\"\"\n",
    "    \n",
    "    def __init__(self, data_list):\n",
    "        self.raw_data = data_list\n",
    "        self.processed_data = []\n",
    "        self.extraction_stats = {}\n",
    "        self.logger = logging.getLogger('financial_data_extractor.DataProcessor')\n",
    "        \n",
    "    def _format_date(self, date_str):\n",
    "        \"\"\"Format a date string to MM/DD/YYYY, handling various input formats\"\"\"\n",
    "        if not date_str or str(date_str).lower() in [\"n/a\", \"na\", \"none\", \"null\"]:\n",
    "            return date_str\n",
    "            \n",
    "        try:\n",
    "            # Parsing with dateutil for flexibility\n",
    "            date_obj = date_parser.parse(date_str, dayfirst=False, yearfirst=False, fuzzy=True)\n",
    "            return date_obj.strftime('%m/%d/%Y')\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not parse date '{date_str}': {str(e)}\")\n",
    "            return date_str\n",
    "    \n",
    "    def _format_currency(self, value_str):\n",
    "        \"\"\"Format a currency value to have 2 decimal places\"\"\"\n",
    "        if not value_str or str(value_str).lower() in [\"n/a\", \"na\", \"none\", \"null\"]:\n",
    "            return value_str\n",
    "            \n",
    "        try:\n",
    "            # Remove any non-numeric characters except decimal point and negative sign\n",
    "            clean_value = re.sub(r'[^\\d.-]', '', str(value_str))\n",
    "            # Format as currency with 2 decimal places\n",
    "            return \"{:.2f}\".format(float(clean_value))\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not process currency value '{value_str}': {str(e)}\")\n",
    "            return value_str\n",
    "    \n",
    "    def _format_percentage(self, value_str):\n",
    "        \"\"\"Format a percentage value to have 2 decimal places\"\"\"\n",
    "        if not value_str or str(value_str).lower() in [\"n/a\", \"na\", \"none\", \"null\"]:\n",
    "            return value_str\n",
    "            \n",
    "        try:\n",
    "            # Remove any non-numeric characters except decimal point and negative sign\n",
    "            clean_value = re.sub(r'[^\\d.-]', '', str(value_str))\n",
    "            # Format as percentage with 2 decimal places\n",
    "            return \"{:.2f}\".format(float(clean_value))\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not process percentage value '{value_str}': {str(e)}\")\n",
    "            return value_str\n",
    "    \n",
    "    def format_data(self):\n",
    "        \"\"\"Format all data according to US standards\"\"\"\n",
    "        self.logger.info(\"Formatting extracted data\")\n",
    "        \n",
    "        for item in self.raw_data:\n",
    "            processed_item = {}\n",
    "            \n",
    "            # Process date fields (MM/DD/YYYY)\n",
    "            for field in [f for f in item.keys() if 'date' in f.lower()]:\n",
    "                if field in item and item[field]:\n",
    "                    processed_item[field] = self._format_date(item[field])\n",
    "            \n",
    "            # Process currency fields (USD format with 2 decimal places)\n",
    "            for field in ['investment_in_original', 'investment_in', 'investment_in_prior']:\n",
    "                if field in item and item[field]:\n",
    "                    processed_item[field] = self._format_currency(item[field])\n",
    "            \n",
    "            # Process yield percentage\n",
    "            if 'yield_percentage' in item and item['yield_percentage']:\n",
    "                processed_item['yield_percentage'] = self._format_percentage(item['yield_percentage'])\n",
    "            \n",
    "            # Copy other fields as is\n",
    "            for field in item.keys():\n",
    "                if field not in processed_item and item[field]:\n",
    "                    processed_item[field] = item[field]\n",
    "            \n",
    "            self.processed_data.append(processed_item)\n",
    "        \n",
    "        self.logger.info(f\"Formatted {len(self.processed_data)} records\")\n",
    "        return self.processed_data\n",
    "    \n",
    "    def calculate_statistics(self):\n",
    "        \"\"\"Calculate extraction statistics\"\"\"\n",
    "        self.logger.info(\"Calculating extraction statistics\")\n",
    "        \n",
    "        total_records = len(self.processed_data)\n",
    "        if total_records == 0:\n",
    "            self.extraction_stats = {\n",
    "                \"total_records\": 0,\n",
    "                \"mandatory_fields_extracted\": 0,\n",
    "                \"mandatory_fields_percentage\": 0,\n",
    "                \"extraction_accuracy\": 0,\n",
    "                \"missing_fields\": CONFIG[\"extraction\"][\"mandatory_fields\"],\n",
    "                \"inconsistent_data\": [],\n",
    "                \"field_presence\": {}\n",
    "            }\n",
    "            return self.extraction_stats\n",
    "        \n",
    "        # Count mandatory fields\n",
    "        mandatory_fields = CONFIG[\"extraction\"][\"mandatory_fields\"]\n",
    "        mandatory_field_counts = {field: 0 for field in mandatory_fields}\n",
    "        \n",
    "        # Track presence of all fields\n",
    "        all_fields = set()\n",
    "        field_presence = {}\n",
    "        \n",
    "        for record in self.processed_data:\n",
    "            record_fields = set(record.keys())\n",
    "            all_fields.update(record_fields)\n",
    "            \n",
    "            for field in mandatory_fields:\n",
    "                if field in record and record[field]:\n",
    "                    mandatory_field_counts[field] += 1\n",
    "        \n",
    "        # Calculate field presence percentages\n",
    "        for field in all_fields:\n",
    "            count = sum(1 for record in self.processed_data if field in record and record[field])\n",
    "            field_presence[field] = {\n",
    "                \"count\": count,\n",
    "                \"percentage\": (count / total_records) * 100\n",
    "            }\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_mandatory_fields = len(mandatory_fields) * total_records\n",
    "        extracted_mandatory_fields = sum(mandatory_field_counts.values())\n",
    "        \n",
    "        mandatory_fields_percentage = (extracted_mandatory_fields / total_mandatory_fields) * 100 if total_mandatory_fields > 0 else 0\n",
    "        \n",
    "        # Identify missing and inconsistent data\n",
    "        missing_fields = []\n",
    "        for field, count in mandatory_field_counts.items():\n",
    "            if count < total_records:\n",
    "                missing_fields.append(f\"{field} ({total_records - count} missing)\")\n",
    "        \n",
    "        # Check for inconsistent data\n",
    "        inconsistent_data = []\n",
    "        \n",
    "        # Example check: Verify if currencies are consistent\n",
    "        currencies = set()\n",
    "        for record in self.processed_data:\n",
    "            if 'currency' in record and record['currency']:\n",
    "                currencies.add(record['currency'])\n",
    "        \n",
    "        if len(currencies) > 1:\n",
    "            inconsistent_data.append(f\"Multiple currencies detected: {', '.join(currencies)}\")\n",
    "        \n",
    "        # Check date formats for consistency\n",
    "        date_formats = set()\n",
    "        for record in self.processed_data:\n",
    "            if 'as_of_date' in record and record['as_of_date']:\n",
    "                date_formats.add(self._identify_date_format(record['as_of_date']))\n",
    "        \n",
    "        if len(date_formats) > 1:\n",
    "            inconsistent_data.append(f\"Multiple date formats detected: {', '.join(date_formats)}\")\n",
    "        \n",
    "        # Calculate overall extraction accuracy (weighted by importance)\n",
    "        # Give more weight to mandatory fields\n",
    "        mandatory_weight = 0.7\n",
    "        additional_weight = 0.3\n",
    "        \n",
    "        mandatory_accuracy = mandatory_fields_percentage\n",
    "        \n",
    "        # Additional fields accuracy (if any are found)\n",
    "        additional_fields = [f for f in all_fields if f not in mandatory_fields]\n",
    "        if additional_fields:\n",
    "            additional_fields_count = sum(field_presence[f][\"count\"] for f in additional_fields)\n",
    "            additional_fields_total = len(additional_fields) * total_records\n",
    "            additional_fields_percentage = (additional_fields_count / additional_fields_total) * 100 if additional_fields_total > 0 else 0\n",
    "            extraction_accuracy = (mandatory_accuracy * mandatory_weight) + (additional_fields_percentage * additional_weight)\n",
    "        else:\n",
    "            extraction_accuracy = mandatory_accuracy\n",
    "        \n",
    "        self.extraction_stats = {\n",
    "            \"total_records\": total_records,\n",
    "            \"mandatory_fields_extracted\": extracted_mandatory_fields,\n",
    "            \"mandatory_fields_percentage\": mandatory_fields_percentage,\n",
    "            \"extraction_accuracy\": extraction_accuracy,\n",
    "            \"missing_fields\": missing_fields,\n",
    "            \"inconsistent_data\": inconsistent_data,\n",
    "            \"field_presence\": field_presence\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"Statistics calculated: {mandatory_fields_percentage:.2f}% mandatory fields extracted\")\n",
    "        return self.extraction_stats\n",
    "    \n",
    "    def _identify_date_format(self, date_str):\n",
    "        \"\"\"Identify the format of a date string\"\"\"\n",
    "        if not date_str or str(date_str).lower() in [\"n/a\", \"na\", \"none\", \"null\"]:\n",
    "            return \"N/A\"\n",
    "            \n",
    "        # Check for MM/DD/YYYY\n",
    "        if re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', date_str):\n",
    "            return \"MM/DD/YYYY\"\n",
    "        \n",
    "        # Check for DD/MM/YYYY\n",
    "        elif re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', date_str):\n",
    "            return \"DD/MM/YYYY\"\n",
    "        \n",
    "        # Check for YYYY-MM-DD\n",
    "        elif re.match(r'\\d{4}-\\d{1,2}-\\d{1,2}', date_str):\n",
    "            return \"YYYY-MM-DD\"\n",
    "        \n",
    "        # Check for Month DD, YYYY\n",
    "        elif re.match(r'[A-Za-z]+ \\d{1,2},?\\s+\\d{4}', date_str):\n",
    "            return \"Month DD, YYYY\"\n",
    "        \n",
    "        return \"Unknown format\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c51598-81c6-49a0-ae59-9bb6f6cbdc75",
   "metadata": {},
   "source": [
    "# **DataStorage Class - Explanation**\r\n",
    "\r\n",
    "The `DataStorage` class is responsible for storing the processed and validated financial data into two output destinations:\r\n",
    "1. A relational SQL database (PostgreSQL or MySQL)\r\n",
    "2. An Excel file with well-formatted data and summary statistics\r\n",
    "\r\n",
    "It also manages schema normalization, logging, formatting, and error handling.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Key Components\r\n",
    "\r\n",
    "### 1. `__init__(self, processed_data, stats)`\r\n",
    "Initializes with:\r\n",
    "- `processed_data`: The final structured records to be stored\r\n",
    "- `stats`: Extraction statistics calculated by the `DataProcessor`\r\n",
    "- Reads database and Excel configuration from `CONFIG`\r\n",
    "- Sets up a logger for tracking\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. `create_dataframe()`\r\n",
    "Converts the list of dictionaries (`processed_data`) into a pandas DataFrame.\r\n",
    "- Ensures all fields across all records are accounted for (even if missing in some rows).\r\n",
    "- Returns a fully normalized DataFrame.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. `store_in_database()`\r\n",
    "Stores the DataFrame in a SQL database:\r\n",
    "- Builds the correct SQLAlchemy connection string based on database type.\r\n",
    "- Drops existing table/view to avoid schema conflicts.\r\n",
    "- Uploads the data using `pandas.DataFrame.to_sql()`.\r\n",
    "- Creates a view `financial_data_stats` with field summary statistics using SQL.\r\n",
    "- Logs success or failure.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. `_create_stats_view(engine, columns)`\r\n",
    "Creates a dynamic SQL view in the database to summarize key statistics:\r\n",
    "- Counts how many non-null values exist for each mandatory field\r\n",
    "- Counts distinct currencies (if applicable)\r\n",
    "- Uses SQL expressions to compute the view dynamically\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 5. `store_in_excel()`\r\n",
    "Writes data and statistics to an Excel workbook:\r\n",
    "- Sheet 1: `Extracted Data` → All normalized records\r\n",
    "- Sheet 2: `Statistics` → Summary of extraction results\r\n",
    "- Also includes per-field presence and quality indicators\r\n",
    "- Calls `_format_excel_file()` to apply formatting for clarity\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 6. `_format_excel_file(writer)`\r\n",
    "Applies visual formatting using `openpyxl`:\r\n",
    "- Bold header cells with gray fill and centered text\r\n",
    "- Adjusts column widths based on content\r\n",
    "- Highlights:\r\n",
    "  - Accuracy and percentage values in green/yellow/red based on thresholds\r\n",
    "  - Missing fields and inconsistencies in red\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Summary\r\n",
    "The `DataStorage` class provides a reliable, production-grade method for exporting cleaned data to both SQL databases and Excel files. It includes validation, formatting, and summarization logic to ensure the outputs are complete, analyzable, and user-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "560cb636-9cdf-42de-9954-e3577509987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStorage:\n",
    "    \"\"\"Store processed data in database and Excel file with error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, processed_data, stats):\n",
    "        self.data = processed_data\n",
    "        self.stats = stats\n",
    "        self.db_config = CONFIG[\"database\"]\n",
    "        self.excel_file = CONFIG[\"output\"][\"excel_file\"]\n",
    "        self.logger = logging.getLogger('financial_data_extractor.DataStorage')\n",
    "        \n",
    "    def create_dataframe(self):\n",
    "        \"\"\"Convert processed data to pandas DataFrame\"\"\"\n",
    "        # Normalize the data to handle missing fields\n",
    "        all_fields = set()\n",
    "        for record in self.data:\n",
    "            all_fields.update(record.keys())\n",
    "        \n",
    "        normalized_data = []\n",
    "        for record in self.data:\n",
    "            normalized_record = {field: record.get(field, None) for field in all_fields}\n",
    "            normalized_data.append(normalized_record)\n",
    "        \n",
    "        return pd.DataFrame(normalized_data)\n",
    "    \n",
    "    def store_in_database(self):\n",
    "        \"\"\"Store data in SQL database with error handling\"\"\"\n",
    "        self.logger.info(f\"Storing data in {self.db_config['type']} database\")\n",
    "        \n",
    "        try:\n",
    "            # Create database connection\n",
    "            if self.db_config[\"type\"] == \"postgresql\":\n",
    "                connection_string = f\"postgresql://{self.db_config['user']}:{self.db_config['password']}@{self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}\"\n",
    "            else:  # MySQL\n",
    "                connection_string = f\"mysql+pymysql://{self.db_config['user']}:{self.db_config['password']}@{self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}\"\n",
    "            \n",
    "            engine = create_engine(connection_string)\n",
    "            \n",
    "            # Drop existing objects to avoid conflicts\n",
    "            from sqlalchemy import text\n",
    "            with engine.connect() as connection:\n",
    "                # Drop the view first, then the table\n",
    "                connection.execute(text(\"DROP VIEW IF EXISTS financial_data_stats CASCADE;\"))\n",
    "                connection.execute(text(\"DROP TABLE IF EXISTS financial_data CASCADE;\"))\n",
    "                connection.commit()\n",
    "            \n",
    "            # Convert data to DataFrame\n",
    "            df = self.create_dataframe()\n",
    "            \n",
    "            # Store in database\n",
    "            df.to_sql('financial_data', engine, if_exists='replace', index=False)\n",
    "            \n",
    "            # Create view for statistics - dynamically based on actual columns\n",
    "            self._create_stats_view(engine, df.columns)\n",
    "            \n",
    "            self.logger.info(f\"Successfully stored {len(df)} records in database\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Database storage error: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _create_stats_view(self, engine, columns):\n",
    "        \"\"\"Create a database view with extraction statistics based on actual columns\"\"\"\n",
    "        try:\n",
    "            # Get the actual column names from the dataframe\n",
    "            column_cases = []\n",
    "            \n",
    "            # Add count for each mandatory field if present in columns\n",
    "            for field in CONFIG[\"extraction\"][\"mandatory_fields\"]:\n",
    "                if field in columns:\n",
    "                    column_cases.append(f\"SUM(CASE WHEN {field} IS NOT NULL THEN 1 ELSE 0 END) AS {field}_count\")\n",
    "            \n",
    "            # Add other useful stats\n",
    "            if \"currency\" in columns:\n",
    "                column_cases.append(\"COUNT(DISTINCT currency) AS currency_count_distinct\")\n",
    "            \n",
    "            # Build the SQL\n",
    "            view_sql = f\"\"\"\n",
    "            CREATE OR REPLACE VIEW financial_data_stats AS\n",
    "            SELECT\n",
    "                COUNT(*) AS total_records,\n",
    "                {', '.join(column_cases)}\n",
    "            FROM financial_data;\n",
    "            \"\"\"\n",
    "            \n",
    "            from sqlalchemy import text\n",
    "            with engine.connect() as connection:\n",
    "                connection.execute(text(view_sql))\n",
    "                connection.commit()\n",
    "            \n",
    "            self.logger.info(\"Successfully created financial_data_stats view\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error creating statistics view: {str(e)}\")\n",
    "    \n",
    "    def store_in_excel(self):\n",
    "        \"\"\"Store data in Excel file with two sheets and formatting\"\"\"\n",
    "        self.logger.info(f\"Storing data in Excel file: {self.excel_file}\")\n",
    "        \n",
    "        try:\n",
    "            # Convert data to DataFrame - handle empty data gracefully\n",
    "            df = self.create_dataframe()\n",
    "            \n",
    "            # Verify we have data to store\n",
    "            if len(df) == 0:\n",
    "                self.logger.warning(\"No data to store in Excel file\")\n",
    "                # Create a dummy dataframe with mandatory columns to avoid Excel creation errors\n",
    "                dummy_columns = CONFIG[\"extraction\"][\"mandatory_fields\"] + CONFIG[\"extraction\"][\"additional_fields\"]\n",
    "                df = pd.DataFrame(columns=dummy_columns)\n",
    "            \n",
    "            # Create a Pandas Excel writer\n",
    "            writer = pd.ExcelWriter(self.excel_file, engine='openpyxl')\n",
    "            \n",
    "            # Write data to \"Extracted Data\" sheet\n",
    "            df.to_excel(writer, sheet_name='Extracted Data', index=False)\n",
    "            \n",
    "            # Create statistics DataFrame\n",
    "            stats_data = {\n",
    "                \"Metric\": [\n",
    "                    \"Total Records Processed\",\n",
    "                    \"Mandatory Fields Extracted\",\n",
    "                    \"Mandatory Fields Percentage\",\n",
    "                    \"Extraction Accuracy\",\n",
    "                    \"Missing Fields\",\n",
    "                    \"Inconsistent Data\"\n",
    "                ],\n",
    "                \"Value\": [\n",
    "                    self.stats[\"total_records\"],\n",
    "                    self.stats[\"mandatory_fields_extracted\"],\n",
    "                    f\"{self.stats['mandatory_fields_percentage']:.2f}%\",\n",
    "                    f\"{self.stats['extraction_accuracy']:.2f}%\",\n",
    "                    \", \".join(self.stats[\"missing_fields\"]) if self.stats[\"missing_fields\"] else \"None\",\n",
    "                    \", \".join(self.stats[\"inconsistent_data\"]) if self.stats[\"inconsistent_data\"] else \"None\"\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Add field presence statistics\n",
    "            if \"field_presence\" in self.stats and self.stats[\"field_presence\"]:\n",
    "                for field, presence in self.stats[\"field_presence\"].items():\n",
    "                    stats_data[\"Metric\"].append(f\"Field presence: {field}\")\n",
    "                    stats_data[\"Value\"].append(f\"{presence['count']} records ({presence['percentage']:.2f}%)\")\n",
    "            \n",
    "            stats_df = pd.DataFrame(stats_data)\n",
    "            \n",
    "            # Write statistics to \"Statistics\" sheet\n",
    "            stats_df.to_excel(writer, sheet_name='Statistics', index=False)\n",
    "            \n",
    "            # Apply formatting to the Excel file\n",
    "            self._format_excel_file(writer)\n",
    "            \n",
    "            # Save the Excel file\n",
    "            writer.close()\n",
    "            \n",
    "            self.logger.info(f\"Successfully stored data in Excel file\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Excel storage error: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _format_excel_file(self, writer):\n",
    "        \"\"\"Apply formatting to the Excel file for better readability\"\"\"\n",
    "        try:\n",
    "            workbook = writer.book\n",
    "            \n",
    "            # Format Data sheet\n",
    "            worksheet = workbook['Extracted Data']\n",
    "            \n",
    "            # Format headers\n",
    "            for col in range(1, worksheet.max_column + 1):\n",
    "                cell = worksheet.cell(row=1, column=col)\n",
    "                cell.font = Font(bold=True)\n",
    "                cell.fill = PatternFill(start_color=\"DDDDDD\", end_color=\"DDDDDD\", fill_type=\"solid\")\n",
    "                cell.alignment = Alignment(horizontal='center')\n",
    "            \n",
    "            # Adjust column widths\n",
    "            for col in range(1, worksheet.max_column + 1):\n",
    "                max_length = 0\n",
    "                column = worksheet.column_dimensions[chr(64 + col)]  # A, B, C, etc.\n",
    "                \n",
    "                # Find the maximum length in the column\n",
    "                for row in range(1, worksheet.max_row + 1):\n",
    "                    cell_value = str(worksheet.cell(row=row, column=col).value or \"\")\n",
    "                    if len(cell_value) > max_length:\n",
    "                        max_length = len(cell_value)\n",
    "                \n",
    "                # Set width with some padding\n",
    "                column.width = max(10, min(50, max_length + 2))\n",
    "            \n",
    "            # Format Statistics sheet\n",
    "            worksheet = workbook['Statistics']\n",
    "            \n",
    "            # Format headers\n",
    "            for col in range(1, worksheet.max_column + 1):\n",
    "                cell = worksheet.cell(row=1, column=col)\n",
    "                cell.font = Font(bold=True)\n",
    "                cell.fill = PatternFill(start_color=\"DDDDDD\", end_color=\"DDDDDD\", fill_type=\"solid\")\n",
    "                cell.alignment = Alignment(horizontal='center')\n",
    "            \n",
    "            # Highlight metrics based on values\n",
    "            for row in range(2, worksheet.max_row + 1):\n",
    "                metric_cell = worksheet.cell(row=row, column=1)\n",
    "                value_cell = worksheet.cell(row=row, column=2)\n",
    "                \n",
    "                # Highlight percentage metrics\n",
    "                if metric_cell.value and \"Percentage\" in str(metric_cell.value) or \"Accuracy\" in str(metric_cell.value):\n",
    "                    value_text = str(value_cell.value or \"\")\n",
    "                    if value_text and \"%\" in value_text:\n",
    "                        try:\n",
    "                            percentage = float(value_text.replace(\"%\", \"\"))\n",
    "                            if percentage < 50:\n",
    "                                value_cell.fill = PatternFill(start_color=\"FFCCCC\", end_color=\"FFCCCC\", fill_type=\"solid\")\n",
    "                            elif percentage < 80:\n",
    "                                value_cell.fill = PatternFill(start_color=\"FFFFCC\", end_color=\"FFFFCC\", fill_type=\"solid\")\n",
    "                            else:\n",
    "                                value_cell.fill = PatternFill(start_color=\"CCFFCC\", end_color=\"CCFFCC\", fill_type=\"solid\")\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                \n",
    "                # Highlight missing fields or inconsistent data\n",
    "                if metric_cell.value and (\"Missing Fields\" in str(metric_cell.value) or \"Inconsistent Data\" in str(metric_cell.value)):\n",
    "                    if value_cell.value and str(value_cell.value) != \"None\":\n",
    "                        value_cell.fill = PatternFill(start_color=\"FFCCCC\", end_color=\"FFCCCC\", fill_type=\"solid\")\n",
    "            \n",
    "            # Adjust column widths\n",
    "            for col in range(1, worksheet.max_column + 1):\n",
    "                max_length = 0\n",
    "                column = worksheet.column_dimensions[chr(64 + col)]  # A, B, C, etc.\n",
    "                \n",
    "                # Find the maximum length in the column\n",
    "                for row in range(1, worksheet.max_row + 1):\n",
    "                    cell_value = str(worksheet.cell(row=row, column=col).value or \"\")\n",
    "                    if len(cell_value) > max_length:\n",
    "                        max_length = len(cell_value)\n",
    "                \n",
    "                # Set width with some padding\n",
    "                column.width = max(15, min(75, max_length + 2))\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error applying Excel formatting: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5d224-4f8a-4abb-ab82-a41e7c4478fe",
   "metadata": {},
   "source": [
    "# **Explanation for create_sample_document() and main()**\r\n",
    "\r\n",
    "This code defines two key functions:\r\n",
    "- `create_sample_document()` – for generating test documents in different formats\r\n",
    "- `main()` – the main controller for the full data extraction and storage pipeline\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Function: `create_sample_document(document_type=\".txt\")`\r\n",
    "Generates a sample financial document in various formats to simulate real-world inputs for testing the pipeline.\r\n",
    "\r\n",
    "### Supported formats:\r\n",
    "- `.txt`: Multi-section plain text with varying field names (e.g., \"Original investment value\", \"Yield rate\")\r\n",
    "- `.csv`: Comma-separated values with clearly labeled headers\r\n",
    "- `.json`: Structured data with a list of investment records\r\n",
    "- `.txt-alt`: An alternate format with different field naming (e.g., \"Cost\", \"Annual Return\", etc.)\r\n",
    "\r\n",
    "### Output:\r\n",
    "- Saves a file with synthetic financial data\r\n",
    "- Returns the path to the created file\r\n",
    "\r\n",
    "This helps test the extractor’s ability to handle variation in field labels and structures.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Function: `main(file_path)`\r\n",
    "Serves as the orchestrator for the entire extraction, transformation, and loading (ETL) process.\r\n",
    "\r\n",
    "### Step-by-step flow:\r\n",
    "\r\n",
    "1. **Extraction**\r\n",
    "   - Initializes `DocumentExtractor`\r\n",
    "   - Extracts raw data from the given file\r\n",
    "   - Ensures it doesn’t fail silently if no data is found\r\n",
    "\r\n",
    "2. **Processing**\r\n",
    "   - Initializes `DataProcessor`\r\n",
    "   - Formats and cleans the data (dates, currencies, percentages)\r\n",
    "   - Calculates statistics about field presence and consistency\r\n",
    "\r\n",
    "3. **Storage**\r\n",
    "   - Initializes `DataStorage`\r\n",
    "   - Saves data to a database and Excel file\r\n",
    "   - Logs results or errors accordingly\r\n",
    "\r\n",
    "4. **Reporting**\r\n",
    "   - Logs summary statistics such as:\r\n",
    "     - Total records\r\n",
    "     - Mandatory field coverage\r\n",
    "     - Extraction accuracy\r\n",
    "     - Any missing or inconsistent values\r\n",
    "\r\n",
    "### Error Handling:\r\n",
    "- Any failure in the process is caught and logged without crashing the program.\r\n",
    "- Returns `True` on success or `False` if an error occurs.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Summary\r\n",
    "These two functions help automate both:\r\n",
    "- The **testing** of the pipeline with realistic variations (`create_sample_document`)\r\n",
    "- The **execution** of the full extraction and reporting process (`main`)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aac1087e-b2f0-42a2-adc3-211d60b0144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_document(document_type=\".txt\"):\n",
    "    \"\"\"\n",
    "    Create a sample financial document for testing with various formats and field variations.\n",
    "    Returns the path to the created document.\n",
    "    \"\"\"\n",
    "    if document_type == \".txt\":\n",
    "        sample_text = \"\"\"# QUARTERLY INVESTMENT REPORT\n",
    "## Confidential Financial Document\n",
    "\n",
    "Report date: 03/31/2024\n",
    "\n",
    "### DETAILED INVESTMENTS\n",
    "\n",
    "#### INVESTMENT 1\n",
    "Security name: Global Technology Fund Class A\n",
    "Original investment value: 400,000.00\n",
    "Current value: 475,250.00\n",
    "Previous value: 425,800.00\n",
    "Currency code: USD\n",
    "Sector: Technology\n",
    "Risk level: Moderate\n",
    "Maturity date: N/A\n",
    "Yield rate: 2.45%\n",
    "\n",
    "#### INVESTMENT 2\n",
    "Instrument name: Emerging Markets ETF\n",
    "Acquisition cost: 200,000.00\n",
    "Market value: 180,500.75\n",
    "Value previous period: 194,325.00\n",
    "Currency: USD\n",
    "Sector: International\n",
    "Risk rating: High\n",
    "Maturity date: N/A\n",
    "Annual yield: 3.85%\n",
    "\n",
    "#### INVESTMENT 3\n",
    "Original security name: US Treasury Bond 2026\n",
    "Initial investment: 250,000.00\n",
    "Present value: 250,000.00\n",
    "Prior investment value: 250,000.00\n",
    "Currency: USD\n",
    "Sector: Government\n",
    "Risk profile: Low\n",
    "Expiration date: 06/15/2026\n",
    "Yield %: 4.25\n",
    "\"\"\"\n",
    "        \n",
    "        # Create a sample text file\n",
    "        sample_file_path = \"sample_financial_document.txt\"\n",
    "        with open(sample_file_path, 'w') as f:\n",
    "            f.write(sample_text)\n",
    "        \n",
    "    elif document_type == \".csv\":\n",
    "        sample_text = \"\"\"Security Name,Initial Investment,Market Value,Prior Value,Currency,Sector,Risk Rating,Maturity Date,Yield %\n",
    "Global Technology Fund Class A,400000.00,475250.00,425800.00,USD,Technology,Moderate,N/A,2.45\n",
    "Emerging Markets ETF,200000.00,180500.75,194325.00,USD,International,High,N/A,3.85\n",
    "US Treasury Bond 2026,250000.00,250000.00,250000.00,USD,Government,Low,06/15/2026,4.25\n",
    "\"\"\"\n",
    "        \n",
    "        # Create a sample CSV file\n",
    "        sample_file_path = \"sample_financial_document.csv\"\n",
    "        with open(sample_file_path, 'w') as f:\n",
    "            f.write(sample_text)\n",
    "            \n",
    "    elif document_type == \".json\":\n",
    "        sample_data = {\n",
    "            \"report_date\": \"03/31/2024\",\n",
    "            \"investments\": [\n",
    "                {\n",
    "                    \"security_name\": \"Global Technology Fund Class A\",\n",
    "                    \"investment_original\": \"400000.00\",\n",
    "                    \"investment_current\": \"475250.00\",\n",
    "                    \"investment_prior\": \"425800.00\",\n",
    "                    \"currency\": \"USD\",\n",
    "                    \"sector\": \"Technology\",\n",
    "                    \"risk_rating\": \"Moderate\",\n",
    "                    \"maturity_date\": \"N/A\",\n",
    "                    \"yield_percentage\": \"2.45\"\n",
    "                },\n",
    "                {\n",
    "                    \"security_name\": \"Emerging Markets ETF\",\n",
    "                    \"investment_original\": \"200000.00\",\n",
    "                    \"investment_current\": \"180500.75\",\n",
    "                    \"investment_prior\": \"194325.00\",\n",
    "                    \"currency\": \"USD\",\n",
    "                    \"sector\": \"International\",\n",
    "                    \"risk_rating\": \"High\",\n",
    "                    \"maturity_date\": \"N/A\",\n",
    "                    \"yield_percentage\": \"3.85\"\n",
    "                },\n",
    "                {\n",
    "                    \"security_name\": \"US Treasury Bond 2026\",\n",
    "                    \"investment_original\": \"250000.00\",\n",
    "                    \"investment_current\": \"250000.00\",\n",
    "                    \"investment_prior\": \"250000.00\",\n",
    "                    \"currency\": \"USD\",\n",
    "                    \"sector\": \"Government\",\n",
    "                    \"risk_rating\": \"Low\",\n",
    "                    \"maturity_date\": \"06/15/2026\",\n",
    "                    \"yield_percentage\": \"4.25\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Create a sample JSON file\n",
    "        sample_file_path = \"sample_financial_document.json\"\n",
    "        with open(sample_file_path, 'w') as f:\n",
    "            json.dump(sample_data, f, indent=2)\n",
    "    \n",
    "    elif document_type == \".txt-alt\":\n",
    "        # Alternative text format with different field names and formatting\n",
    "        sample_text = \"\"\"INVESTMENT PORTFOLIO SUMMARY\n",
    "=========================\n",
    "Statement Date: March 31, 2024\n",
    "\n",
    "INVESTMENT HOLDINGS\n",
    "------------------\n",
    "\n",
    "1. ASSET: Global Technology Fund Class A\n",
    "   Cost: $400,000.00\n",
    "   Market Value (Current): $475,250.00\n",
    "   Market Value (Previous Period): $425,800.00\n",
    "   Traded in: USD\n",
    "   Industry Sector: Technology\n",
    "   Risk Assessment: Moderate\n",
    "   Term End Date: Not Applicable\n",
    "   Annual Return: 2.45%\n",
    "\n",
    "2. ASSET: Emerging Markets ETF\n",
    "   Cost: $200,000.00\n",
    "   Market Value (Current): $180,500.75\n",
    "   Market Value (Previous Period): $194,325.00\n",
    "   Traded in: USD\n",
    "   Industry Sector: International\n",
    "   Risk Assessment: High\n",
    "   Term End Date: Not Applicable\n",
    "   Annual Return: 3.85%\n",
    "\n",
    "3. ASSET: US Treasury Bond 2026\n",
    "   Cost: $250,000.00\n",
    "   Market Value (Current): $250,000.00\n",
    "   Market Value (Previous Period): $250,000.00\n",
    "   Traded in: USD\n",
    "   Industry Sector: Government\n",
    "   Risk Assessment: Low\n",
    "   Term End Date: 15/06/2026\n",
    "   Annual Return: 4.25%\n",
    "\"\"\"\n",
    "        \n",
    "        # Create an alternative sample text file\n",
    "        sample_file_path = \"sample_financial_document_alt.txt\"\n",
    "        with open(sample_file_path, 'w') as f:\n",
    "            f.write(sample_text)\n",
    "    \n",
    "    else:\n",
    "        # Create a simple txt file as fallback\n",
    "        sample_file_path = \"sample_financial_document.txt\"\n",
    "        with open(sample_file_path, 'w') as f:\n",
    "            f.write(\"As of date: 03/31/2024\\nAsset: Sample Asset\\nInvestment (original): 100000\\nInvestment: 120000\\nInvestment (prior): 110000\\nCurrency: USD\")\n",
    "    \n",
    "    return sample_file_path\n",
    "\n",
    "\n",
    "def main(file_path):\n",
    "    \"\"\"Main function to orchestrate the extraction, processing and storage\"\"\"\n",
    "    logger = logging.getLogger('financial_data_extractor.main')\n",
    "    logger.info(f\"Starting extraction process for {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract data from document\n",
    "        logger.info(f\"Extracting data from {file_path}...\")\n",
    "        extractor = DocumentExtractor(file_path)\n",
    "        raw_data = extractor.extract_data()\n",
    "        \n",
    "        if not raw_data:\n",
    "            logger.warning(\"No data was extracted from the document.\")\n",
    "            # Create empty data and continue to generate statistics\n",
    "            raw_data = [{}]\n",
    "        \n",
    "        logger.info(f\"Extracted {len(raw_data)} records.\")\n",
    "        \n",
    "        # Step 2: Process and format data\n",
    "        logger.info(\"Processing and formatting data...\")\n",
    "        processor = DataProcessor(raw_data)\n",
    "        processed_data = processor.format_data()\n",
    "        stats = processor.calculate_statistics()\n",
    "        \n",
    "        # Step 3: Store data\n",
    "        logger.info(\"Storing data...\")\n",
    "        storage = DataStorage(processed_data, stats)\n",
    "        \n",
    "        # Attempt to store in database\n",
    "        db_result = storage.store_in_database()\n",
    "        if db_result:\n",
    "            logger.info(f\"Data successfully stored in {CONFIG['database']['type']} database.\")\n",
    "        else:\n",
    "            logger.warning(f\"Failed to store data in database. Check your database connection settings.\")\n",
    "        \n",
    "        # Store in Excel\n",
    "        excel_result = storage.store_in_excel()\n",
    "        if excel_result:\n",
    "            logger.info(f\"Data successfully stored in Excel file: {CONFIG['output']['excel_file']}\")\n",
    "        else:\n",
    "            logger.warning(\"Failed to store data in Excel file.\")\n",
    "        \n",
    "        # Step 4: Print statistics\n",
    "        logger.info(\"\\nExtraction Statistics:\")\n",
    "        logger.info(f\"Total Records: {stats['total_records']}\")\n",
    "        logger.info(f\"Mandatory Fields Extracted: {stats['mandatory_fields_extracted']}\")\n",
    "        logger.info(f\"Mandatory Fields Percentage: {stats['mandatory_fields_percentage']:.2f}%\")\n",
    "        logger.info(f\"Extraction Accuracy: {stats['extraction_accuracy']:.2f}%\")\n",
    "        \n",
    "        if stats['missing_fields']:\n",
    "            logger.info(f\"Missing Fields: {', '.join(stats['missing_fields'])}\")\n",
    "        else:\n",
    "            logger.info(\"Missing Fields: None\")\n",
    "        \n",
    "        if stats['inconsistent_data']:\n",
    "            logger.info(f\"Inconsistent Data: {', '.join(stats['inconsistent_data'])}\")\n",
    "        else:\n",
    "            logger.info(\"Inconsistent Data: None\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in extraction process: {str(e)}\", exc_info=True)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e3351-4d2c-410d-b3be-5f51fdbcef9f",
   "metadata": {},
   "source": [
    "# **Explanation for Test Text Document Extraction**\r\n",
    "\r\n",
    "This code cell demonstrates how to test the document extraction pipeline using a sample plain text (`.txt`) financial report.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Steps Explained\r\n",
    "\r\n",
    "### 1. Create a Sample Text Document\r\n",
    "```python\r\n",
    "sample_path_txt = create_sample_document(\".txt\")\r\n",
    "```\r\n",
    "- Calls the helper function to generate a `.txt` file containing multiple investment entries.\r\n",
    "- These entries contain variations of field labels (e.g., \"Security name\", \"Currency code\").\r\n",
    "- Returns the path to the newly created sample file.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. Extract Data\r\n",
    "```python\r\n",
    "extractor = DocumentExtractor(sample_path_txt)\r\n",
    "extracted_data = extractor.extract_data()\r\n",
    "```\r\n",
    "- Instantiates the `DocumentExtractor` with the text file.\r\n",
    "- Parses the file, applies regex patterns, and attempts to extract meaningful investment records.\r\n",
    "- Output: A list of dictionaries, one per investment.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. Display Extracted Raw Data\r\n",
    "```python\r\n",
    "for i, item in enumerate(extracted_data):\r\n",
    "    ...\r\n",
    "```\r\n",
    "- Prints each raw record that was extracted before any formatting is applied.\r\n",
    "- Helps verify if field matching worked correctly.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. Format and Process Data\r\n",
    "```python\r\n",
    "processor = DataProcessor(extracted_data)\r\n",
    "processed_data = processor.format_data()\r\n",
    "stats = processor.calculate_statistics()\r\n",
    "```\r\n",
    "- Cleans values: formats dates, normalizes currency and percentage fields.\r\n",
    "- Computes statistics about the presence of required fields and data quality.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 5. Display Processed Data\r\n",
    "```python\r\n",
    "for i, item in enumerate(processed_data):\r\n",
    "    ...\r\n",
    "```\r\n",
    "- Shows formatted records after transformation.\r\n",
    "- Ensures values are in standardized formats.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 6. Show Extraction Statistics\r\n",
    "```python\r\n",
    "print(f\"Total Records: {stats['total_records']}\")\r\n",
    "...\r\n",
    "```\r\n",
    "- Outputs:\r\n",
    "  - Record count\r\n",
    "  - Percentage of mandatory fields successfully extracted\r\n",
    "  - Overall extraction accuracy\r\n",
    "  - Any missing fields or inconsistencies (e.g., multiple currencies or date formats)\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Summary\r\n",
    "This cell provides a complete test run of the text-based extraction pipeline:\r\n",
    "- Simulates a real-world financial document with field variation\r\n",
    "- Evaluates the extractor's ability to identify and map field values\r\n",
    "- Processes and validates the output\r\n",
    "- Logs results and identifies issues with the data\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1993b80a-f496-4672-9c26-540bef98c34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 17:01:46,358 - financial_data_extractor.DocumentExtractor - INFO - Extracting text from sample_financial_document.txt (type: .txt)\n",
      "2025-05-09 17:01:46,359 - financial_data_extractor.DocumentExtractor - INFO - Successfully extracted 856 characters of text\n",
      "2025-05-09 17:01:46,360 - financial_data_extractor.DocumentExtractor - INFO - Starting data extraction\n",
      "2025-05-09 17:01:46,362 - financial_data_extractor.DocumentExtractor - INFO - Global as_of_date extracted: 03/31/2024\n",
      "2025-05-09 17:01:46,362 - financial_data_extractor.DocumentExtractor - INFO - No tabular data found, trying section-based extraction\n",
      "2025-05-09 17:01:46,362 - financial_data_extractor.DocumentExtractor - INFO - Identified 3 investment sections\n",
      "2025-05-09 17:01:46,367 - financial_data_extractor.DocumentExtractor - INFO - Extracted 3 valid records with data\n",
      "2025-05-09 17:01:46,367 - financial_data_extractor.DataProcessor - INFO - Formatting extracted data\n",
      "2025-05-09 17:01:46,368 - financial_data_extractor.DataProcessor - INFO - Formatted 3 records\n",
      "2025-05-09 17:01:46,368 - financial_data_extractor.DataProcessor - INFO - Calculating extraction statistics\n",
      "2025-05-09 17:01:46,369 - financial_data_extractor.DataProcessor - INFO - Statistics calculated: 100.00% mandatory fields extracted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample document: sample_financial_document.txt\n",
      "\n",
      "--- Extracted Raw Data (Text) ---\n",
      "\n",
      "Record 1:\n",
      "  as_of_date: 03/31/2024\n",
      "  original_security_name: Global Technology Fund Class A\n",
      "  investment_in_original: 400000.00\n",
      "  investment_in: 475250.00\n",
      "  investment_in_prior: 425800.00\n",
      "  currency: USD\n",
      "  sector: Technology\n",
      "  risk_rating: Moderate\n",
      "  maturity_date: N/A\n",
      "  yield_percentage: 2.45\n",
      "\n",
      "Record 2:\n",
      "  as_of_date: 03/31/2024\n",
      "  original_security_name: Emerging Markets ETF\n",
      "  investment_in_original: 200000.00\n",
      "  investment_in: 180500.75\n",
      "  investment_in_prior: 194325.00\n",
      "  currency: USD\n",
      "  sector: International\n",
      "  risk_rating: High\n",
      "  maturity_date: N/A\n",
      "  yield_percentage: 3.85\n",
      "\n",
      "Record 3:\n",
      "  as_of_date: 03/31/2024\n",
      "  original_security_name: US Treasury Bond 2026\n",
      "  investment_in_original: 250000.00\n",
      "  investment_in: 250000.00\n",
      "  investment_in_prior: 250000.00\n",
      "  currency: USD\n",
      "  sector: Government\n",
      "  risk_rating: Low\n",
      "  maturity_date: 06/15/2026\n",
      "  yield_percentage: 4.25\n",
      "\n",
      "--- Processed Data (Text) ---\n",
      "\n",
      "Record 1:\n",
      "  as_of_date: 03/31/2024\n",
      "  maturity_date: N/A\n",
      "  investment_in_original: 400000.00\n",
      "  investment_in: 475250.00\n",
      "  investment_in_prior: 425800.00\n",
      "  yield_percentage: 2.45\n",
      "  original_security_name: Global Technology Fund Class A\n",
      "  currency: USD\n",
      "  sector: Technology\n",
      "  risk_rating: Moderate\n",
      "\n",
      "Record 2:\n",
      "  as_of_date: 03/31/2024\n",
      "  maturity_date: N/A\n",
      "  investment_in_original: 200000.00\n",
      "  investment_in: 180500.75\n",
      "  investment_in_prior: 194325.00\n",
      "  yield_percentage: 3.85\n",
      "  original_security_name: Emerging Markets ETF\n",
      "  currency: USD\n",
      "  sector: International\n",
      "  risk_rating: High\n",
      "\n",
      "Record 3:\n",
      "  as_of_date: 03/31/2024\n",
      "  maturity_date: 06/15/2026\n",
      "  investment_in_original: 250000.00\n",
      "  investment_in: 250000.00\n",
      "  investment_in_prior: 250000.00\n",
      "  yield_percentage: 4.25\n",
      "  original_security_name: US Treasury Bond 2026\n",
      "  currency: USD\n",
      "  sector: Government\n",
      "  risk_rating: Low\n",
      "\n",
      "--- Extraction Statistics (Text) ---\n",
      "Total Records: 3\n",
      "Mandatory Fields Extracted: 18\n",
      "Mandatory Fields Percentage: 100.00%\n",
      "Extraction Accuracy: 100.00%\n",
      "Missing Fields: None\n",
      "Inconsistent Data: None\n"
     ]
    }
   ],
   "source": [
    "# Create and test with a sample text document\n",
    "sample_path_txt = create_sample_document(\".txt\")\n",
    "print(f\"Created sample document: {sample_path_txt}\")\n",
    "\n",
    "# Test extraction with the sample document\n",
    "extractor = DocumentExtractor(sample_path_txt)\n",
    "extracted_data = extractor.extract_data()\n",
    "\n",
    "print(\"\\n--- Extracted Raw Data (Text) ---\")\n",
    "for i, item in enumerate(extracted_data):\n",
    "    print(f\"\\nRecord {i+1}:\")\n",
    "    for key, value in item.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Process the extracted data\n",
    "processor = DataProcessor(extracted_data)\n",
    "processed_data = processor.format_data()\n",
    "stats = processor.calculate_statistics()\n",
    "\n",
    "print(\"\\n--- Processed Data (Text) ---\")\n",
    "for i, item in enumerate(processed_data):\n",
    "    print(f\"\\nRecord {i+1}:\")\n",
    "    for key, value in item.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n--- Extraction Statistics (Text) ---\")\n",
    "print(f\"Total Records: {stats['total_records']}\")\n",
    "print(f\"Mandatory Fields Extracted: {stats['mandatory_fields_extracted']}\")\n",
    "print(f\"Mandatory Fields Percentage: {stats['mandatory_fields_percentage']:.2f}%\")\n",
    "print(f\"Extraction Accuracy: {stats['extraction_accuracy']:.2f}%\")\n",
    "\n",
    "if stats['missing_fields']:\n",
    "    print(f\"Missing Fields: {', '.join(stats['missing_fields'])}\")\n",
    "else:\n",
    "    print(\"Missing Fields: None\")\n",
    "\n",
    "if stats['inconsistent_data']:\n",
    "    print(f\"Inconsistent Data: {', '.join(stats['inconsistent_data'])}\")\n",
    "else:\n",
    "    print(\"Inconsistent Data: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7291e-4ee8-40ad-bddb-5630bd7cca28",
   "metadata": {},
   "source": [
    "# **Explanation for Test CSV Document Extraction**\r\n",
    "\r\n",
    "This code cell tests the full data extraction and processing pipeline using a sample `.csv` financial document.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Steps Explained\r\n",
    "\r\n",
    "### 1. Create a Sample CSV Document\r\n",
    "```python\r\n",
    "sample_path_csv = create_sample_document(\".csv\")\r\n",
    "```\r\n",
    "- Uses the `create_sample_document()` function to generate a synthetic CSV file with realistic financial investment data.\r\n",
    "- Stores the path of the created file in `sample_path_csv`.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. Extract Data\r\n",
    "```python\r\n",
    "extractor = DocumentExtractor(sample_path_csv)\r\n",
    "extracted_data = extractor.extract_data()\r\n",
    "```\r\n",
    "- Initializes the `DocumentExtractor` with the path to the CSV file.\r\n",
    "- Calls `extract_data()` to parse and extract structured records.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. Print Raw Extracted Data\r\n",
    "```python\r\n",
    "for i, item in enumerate(extracted_data):\r\n",
    "    ...\r\n",
    "```\r\n",
    "- Iterates over each extracted record and prints its fields.\r\n",
    "- Helps verify that the extractor correctly mapped CSV columns to standard field names.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. Process and Format the Data\r\n",
    "```python\r\n",
    "processor = DataProcessor(extracted_data)\r\n",
    "processed_data = processor.format_data()\r\n",
    "stats = processor.calculate_statistics()\r\n",
    "```\r\n",
    "- Initializes the `DataProcessor` with the extracted data.\r\n",
    "- Formats dates, currency, and percentages using standardized rules.\r\n",
    "- Calculates statistics on the completeness and consistency of the extracted data.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 5. Print Processed Data\r\n",
    "```python\r\n",
    "for i, item in enumerate(processed_data):\r\n",
    "    ...\r\n",
    "```\r\n",
    "- Prints the cleaned and formatted data record-by-record.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 6. Print Extraction Statistics\r\n",
    "```python\r\n",
    "print(f\"Total Records: {stats['total_records']}\")\r\n",
    "...\r\n",
    "```\r\n",
    "- Displays a summary of the extraction performance, including:\r\n",
    "  - Total records processed\r\n",
    "  - Number and percentage of mandatory fields extracted\r\n",
    "  - Overall extraction accuracy\r\n",
    "  - Any missing or inconsistent fields\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Summary\r\n",
    "This code cell demonstrates and validates the CSV data extraction pipeline:\r\n",
    "- It confirms that the `DocumentExtractor` correctly reads and maps CSV fields.\r\n",
    "- It shows that the `DataProcessor` properly formats and evaluates the data.\r\n",
    "- It produces a useful summary of data quality for debugging or reporting purposes.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b4f3c34-73a9-41b4-b0b9-ca02f6c2b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 17:01:47,796 - financial_data_extractor.DocumentExtractor - INFO - Extracting text from sample_financial_document.csv (type: .csv)\n",
      "2025-05-09 17:01:47,799 - financial_data_extractor.DocumentExtractor - INFO - Successfully extracted 372 characters of text\n",
      "2025-05-09 17:01:47,799 - financial_data_extractor.DocumentExtractor - INFO - Starting data extraction\n",
      "2025-05-09 17:01:47,801 - financial_data_extractor.DocumentExtractor - INFO - Global as_of_date extracted: None\n",
      "2025-05-09 17:01:47,801 - financial_data_extractor.DocumentExtractor - INFO - No tabular data found, trying section-based extraction\n",
      "2025-05-09 17:01:47,803 - financial_data_extractor.DocumentExtractor - INFO - Identified 1 investment sections\n",
      "2025-05-09 17:01:47,803 - financial_data_extractor.DocumentExtractor - INFO - No investment sections found, trying general extraction\n",
      "2025-05-09 17:01:47,805 - financial_data_extractor.DocumentExtractor - INFO - Extracted 3 records from CSV file\n",
      "2025-05-09 17:01:47,805 - financial_data_extractor.DocumentExtractor - INFO - Extracted 3 valid records with data\n",
      "2025-05-09 17:01:47,806 - financial_data_extractor.DataProcessor - INFO - Formatting extracted data\n",
      "2025-05-09 17:01:47,807 - financial_data_extractor.DataProcessor - INFO - Formatted 3 records\n",
      "2025-05-09 17:01:47,807 - financial_data_extractor.DataProcessor - INFO - Calculating extraction statistics\n",
      "2025-05-09 17:01:47,808 - financial_data_extractor.DataProcessor - INFO - Statistics calculated: 83.33% mandatory fields extracted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample document: sample_financial_document.csv\n",
      "\n",
      "--- Extracted Raw Data (CSV) ---\n",
      "\n",
      "Record 1:\n",
      "  original_security_name: Global Technology Fund Class A\n",
      "  investment_in_original: 400000.00\n",
      "  investment_in: 475250.00\n",
      "  investment_in_prior: 425800.00\n",
      "  currency: USD\n",
      "  sector: Technology\n",
      "  risk_rating: Moderate\n",
      "  maturity_date: N/A\n",
      "  yield_percentage: 2.45\n",
      "\n",
      "Record 2:\n",
      "  original_security_name: Emerging Markets ETF\n",
      "  investment_in_original: 200000.00\n",
      "  investment_in: 180500.75\n",
      "  investment_in_prior: 194325.00\n",
      "  currency: USD\n",
      "  sector: International\n",
      "  risk_rating: High\n",
      "  maturity_date: N/A\n",
      "  yield_percentage: 3.85\n",
      "\n",
      "Record 3:\n",
      "  original_security_name: US Treasury Bond 2026\n",
      "  investment_in_original: 250000.00\n",
      "  investment_in: 250000.00\n",
      "  investment_in_prior: 250000.00\n",
      "  currency: USD\n",
      "  sector: Government\n",
      "  risk_rating: Low\n",
      "  maturity_date: 06/15/2026\n",
      "  yield_percentage: 4.25\n",
      "\n",
      "--- Processed Data (CSV) ---\n",
      "\n",
      "Record 1:\n",
      "  maturity_date: N/A\n",
      "  investment_in_original: 400000.00\n",
      "  investment_in: 475250.00\n",
      "  investment_in_prior: 425800.00\n",
      "  yield_percentage: 2.45\n",
      "  original_security_name: Global Technology Fund Class A\n",
      "  currency: USD\n",
      "  sector: Technology\n",
      "  risk_rating: Moderate\n",
      "\n",
      "Record 2:\n",
      "  maturity_date: N/A\n",
      "  investment_in_original: 200000.00\n",
      "  investment_in: 180500.75\n",
      "  investment_in_prior: 194325.00\n",
      "  yield_percentage: 3.85\n",
      "  original_security_name: Emerging Markets ETF\n",
      "  currency: USD\n",
      "  sector: International\n",
      "  risk_rating: High\n",
      "\n",
      "Record 3:\n",
      "  maturity_date: 06/15/2026\n",
      "  investment_in_original: 250000.00\n",
      "  investment_in: 250000.00\n",
      "  investment_in_prior: 250000.00\n",
      "  yield_percentage: 4.25\n",
      "  original_security_name: US Treasury Bond 2026\n",
      "  currency: USD\n",
      "  sector: Government\n",
      "  risk_rating: Low\n",
      "\n",
      "--- Extraction Statistics (CSV) ---\n",
      "Total Records: 3\n",
      "Mandatory Fields Extracted: 15\n",
      "Mandatory Fields Percentage: 83.33%\n",
      "Extraction Accuracy: 88.33%\n",
      "Missing Fields: as_of_date (3 missing)\n",
      "Inconsistent Data: None\n"
     ]
    }
   ],
   "source": [
    "# Create and test with a sample CSV document\n",
    "sample_path_csv = create_sample_document(\".csv\")\n",
    "print(f\"Created sample document: {sample_path_csv}\")\n",
    "\n",
    "# Test extraction with the sample document\n",
    "extractor = DocumentExtractor(sample_path_csv)\n",
    "extracted_data = extractor.extract_data()\n",
    "\n",
    "print(\"\\n--- Extracted Raw Data (CSV) ---\")\n",
    "for i, item in enumerate(extracted_data):\n",
    "    print(f\"\\nRecord {i+1}:\")\n",
    "    for key, value in item.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Process the extracted data\n",
    "processor = DataProcessor(extracted_data)\n",
    "processed_data = processor.format_data()\n",
    "stats = processor.calculate_statistics()\n",
    "\n",
    "print(\"\\n--- Processed Data (CSV) ---\")\n",
    "for i, item in enumerate(processed_data):\n",
    "    print(f\"\\nRecord {i+1}:\")\n",
    "    for key, value in item.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n--- Extraction Statistics (CSV) ---\")\n",
    "print(f\"Total Records: {stats['total_records']}\")\n",
    "print(f\"Mandatory Fields Extracted: {stats['mandatory_fields_extracted']}\")\n",
    "print(f\"Mandatory Fields Percentage: {stats['mandatory_fields_percentage']:.2f}%\")\n",
    "print(f\"Extraction Accuracy: {stats['extraction_accuracy']:.2f}%\")\n",
    "\n",
    "if stats['missing_fields']:\n",
    "    print(f\"Missing Fields: {', '.join(stats['missing_fields'])}\")\n",
    "else:\n",
    "    print(\"Missing Fields: None\")\n",
    "\n",
    "if stats['inconsistent_data']:\n",
    "    print(f\"Inconsistent Data: {', '.join(stats['inconsistent_data'])}\")\n",
    "else:\n",
    "    print(\"Inconsistent Data: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4e145-9126-4494-898a-e5de94ef9bcb",
   "metadata": {},
   "source": [
    "# **Explanation for JSON Document Extraction**\r\n",
    "\r\n",
    "This code cell tests how the pipeline handles structured input in JSON format, commonly used in APIs and automated reporting.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Steps Explained\r\n",
    "\r\n",
    "### 1. Create a Sample JSON Document\r\n",
    "```python\r\n",
    "sample_path_json = create_sample_document(\".json\")\r\n",
    "```\r\n",
    "- Calls `create_sample_document()` to generate a JSON file with nested investment data.\r\n",
    "- Each investment record includes fields like `security_name`, `investment_original`, `currency`, etc.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. Extract Data from JSON\r\n",
    "```python\r\n",
    "extractor = DocumentExtractor(sample_path_json)\r\n",
    "extracted_data = extractor.extract_data()\r\n",
    "```\r\n",
    "- Instantiates the `DocumentExtractor` and parses the JSON content.\r\n",
    "- Converts structured JSON into a list of dictionaries, one per investment.\r\n",
    "- Uses predefined field mappings to match internal field names.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. Print Extracted Raw Data\r\n",
    "```python\r\n",
    "for i, item in enumerate(extracted_data):\r\n",
    "    ...\r\n",
    "```\r\n",
    "- Displays each raw record before formatting.\r\n",
    "- Useful for verifying whether field values and names were captured correctly from the JSON structure.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. Format and Process the Extracted Data\r\n",
    "```python\r\n",
    "processor = DataProcessor(extracted_data)\r\n",
    "processed_data = processor.format_data()\r\n",
    "stats = processor.calculate_statistics()\r\n",
    "```\r\n",
    "- Applies formatting rules to standardize dates, currencies, and percentages.\r\n",
    "- Computes summary statistics including field completeness and accuracy.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 5. Display Processed Data\r\n",
    "```python\r\n",
    "for i, item in enumerate(processed_data):\r\n",
    "    ...\r\n",
    "```\r\n",
    "- Shows the cleaned and normalized records.\r\n",
    "- Each record should now follow consistent formatting rules.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 6. Print Extraction Statistics\r\n",
    "```python\r\n",
    "print(f\"Total Records: {stats['total_records']}\")\r\n",
    "...\r\n",
    "```\r\n",
    "- Summarizes the quality of extraction, including:\r\n",
    "  - Total records processed\r\n",
    "  - How many mandatory fields were successfully extracted\r\n",
    "  - Percentage completeness and accuracy\r\n",
    "  - Any missing or inconsistent fields detected\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Summary\r\n",
    "This cell demonstrates how the pipeline processes well-structured JSON input. It validates:\r\n",
    "- That the `DocumentExtractor` can handle nested JSON formats\r\n",
    "- That data is properly mapped and cleaned\r\n",
    "- That statistics provide clear insight into extraction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20be5e40-d0ee-4b55-a534-0cf8ef57e0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 17:01:59,210 - financial_data_extractor.DocumentExtractor - INFO - Extracting text from sample_financial_document.json (type: .json)\n",
      "2025-05-09 17:01:59,212 - financial_data_extractor.DocumentExtractor - INFO - Successfully extracted 1067 characters of text\n",
      "2025-05-09 17:01:59,213 - financial_data_extractor.DocumentExtractor - INFO - Starting data extraction\n",
      "2025-05-09 17:01:59,214 - financial_data_extractor.DocumentExtractor - INFO - Global as_of_date extracted: None\n",
      "2025-05-09 17:01:59,214 - financial_data_extractor.DocumentExtractor - INFO - No tabular data found, trying section-based extraction\n",
      "2025-05-09 17:01:59,216 - financial_data_extractor.DocumentExtractor - INFO - Identified 1 investment sections\n",
      "2025-05-09 17:01:59,216 - financial_data_extractor.DocumentExtractor - INFO - No investment sections found, trying general extraction\n",
      "2025-05-09 17:01:59,217 - financial_data_extractor.DocumentExtractor - INFO - Extracted 3 records from JSON file\n",
      "2025-05-09 17:01:59,219 - financial_data_extractor.DocumentExtractor - INFO - Extracted 3 valid records with data\n",
      "2025-05-09 17:01:59,219 - financial_data_extractor.DataProcessor - INFO - Formatting extracted data\n",
      "2025-05-09 17:01:59,220 - financial_data_extractor.DataProcessor - INFO - Formatted 3 records\n",
      "2025-05-09 17:01:59,220 - financial_data_extractor.DataProcessor - INFO - Calculating extraction statistics\n",
      "2025-05-09 17:01:59,221 - financial_data_extractor.DataProcessor - INFO - Statistics calculated: 100.00% mandatory fields extracted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample document: sample_financial_document.json\n",
      "\n",
      "--- Extracted Raw Data (JSON) ---\n",
      "\n",
      "Record 1:\n",
      "  as_of_date: 03/31/2024\n",
      "  original_security_name: Global Technology Fund Class A\n",
      "  investment_in_original: 400000.00\n",
      "  investment_in: 475250.00\n",
      "  investment_in_prior: 425800.00\n",
      "  currency: USD\n",
      "  sector: Technology\n",
      "  risk_rating: Moderate\n",
      "  maturity_date: N/A\n",
      "  yield_percentage: 2.45\n",
      "\n",
      "Record 2:\n",
      "  as_of_date: 03/31/2024\n",
      "  original_security_name: Emerging Markets ETF\n",
      "  investment_in_original: 200000.00\n",
      "  investment_in: 180500.75\n",
      "  investment_in_prior: 194325.00\n",
      "  currency: USD\n",
      "  sector: International\n",
      "  risk_rating: High\n",
      "  maturity_date: N/A\n",
      "  yield_percentage: 3.85\n",
      "\n",
      "Record 3:\n",
      "  as_of_date: 03/31/2024\n",
      "  original_security_name: US Treasury Bond 2026\n",
      "  investment_in_original: 250000.00\n",
      "  investment_in: 250000.00\n",
      "  investment_in_prior: 250000.00\n",
      "  currency: USD\n",
      "  sector: Government\n",
      "  risk_rating: Low\n",
      "  maturity_date: 06/15/2026\n",
      "  yield_percentage: 4.25\n",
      "\n",
      "--- Processed Data (JSON) ---\n",
      "\n",
      "Record 1:\n",
      "  as_of_date: 03/31/2024\n",
      "  maturity_date: N/A\n",
      "  investment_in_original: 400000.00\n",
      "  investment_in: 475250.00\n",
      "  investment_in_prior: 425800.00\n",
      "  yield_percentage: 2.45\n",
      "  original_security_name: Global Technology Fund Class A\n",
      "  currency: USD\n",
      "  sector: Technology\n",
      "  risk_rating: Moderate\n",
      "\n",
      "Record 2:\n",
      "  as_of_date: 03/31/2024\n",
      "  maturity_date: N/A\n",
      "  investment_in_original: 200000.00\n",
      "  investment_in: 180500.75\n",
      "  investment_in_prior: 194325.00\n",
      "  yield_percentage: 3.85\n",
      "  original_security_name: Emerging Markets ETF\n",
      "  currency: USD\n",
      "  sector: International\n",
      "  risk_rating: High\n",
      "\n",
      "Record 3:\n",
      "  as_of_date: 03/31/2024\n",
      "  maturity_date: 06/15/2026\n",
      "  investment_in_original: 250000.00\n",
      "  investment_in: 250000.00\n",
      "  investment_in_prior: 250000.00\n",
      "  yield_percentage: 4.25\n",
      "  original_security_name: US Treasury Bond 2026\n",
      "  currency: USD\n",
      "  sector: Government\n",
      "  risk_rating: Low\n",
      "\n",
      "--- Extraction Statistics (JSON) ---\n",
      "Total Records: 3\n",
      "Mandatory Fields Extracted: 18\n",
      "Mandatory Fields Percentage: 100.00%\n",
      "Extraction Accuracy: 100.00%\n",
      "Missing Fields: None\n",
      "Inconsistent Data: None\n"
     ]
    }
   ],
   "source": [
    "# Create and test with a sample JSON document\n",
    "sample_path_json = create_sample_document(\".json\")\n",
    "print(f\"Created sample document: {sample_path_json}\")\n",
    "\n",
    "# Test extraction with the sample document\n",
    "extractor = DocumentExtractor(sample_path_json)\n",
    "extracted_data = extractor.extract_data()\n",
    "\n",
    "print(\"\\n--- Extracted Raw Data (JSON) ---\")\n",
    "for i, item in enumerate(extracted_data):\n",
    "    print(f\"\\nRecord {i+1}:\")\n",
    "    for key, value in item.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Process the extracted data\n",
    "processor = DataProcessor(extracted_data)\n",
    "processed_data = processor.format_data()\n",
    "stats = processor.calculate_statistics()\n",
    "\n",
    "print(\"\\n--- Processed Data (JSON) ---\")\n",
    "for i, item in enumerate(processed_data):\n",
    "    print(f\"\\nRecord {i+1}:\")\n",
    "    for key, value in item.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n--- Extraction Statistics (JSON) ---\")\n",
    "print(f\"Total Records: {stats['total_records']}\")\n",
    "print(f\"Mandatory Fields Extracted: {stats['mandatory_fields_extracted']}\")\n",
    "print(f\"Mandatory Fields Percentage: {stats['mandatory_fields_percentage']:.2f}%\")\n",
    "print(f\"Extraction Accuracy: {stats['extraction_accuracy']:.2f}%\")\n",
    "\n",
    "if stats['missing_fields']:\n",
    "    print(f\"Missing Fields: {', '.join(stats['missing_fields'])}\")\n",
    "else:\n",
    "    print(\"Missing Fields: None\")\n",
    "\n",
    "if stats['inconsistent_data']:\n",
    "    print(f\"Inconsistent Data: {', '.join(stats['inconsistent_data'])}\")\n",
    "else:\n",
    "    print(\"Inconsistent Data: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca80eb0-bd40-4a92-b5c2-73af9ebee992",
   "metadata": {},
   "source": [
    "# **Explanation for Full Pipeline Test**\r\n",
    "\r\n",
    "This code cell performs an end-to-end test of the entire data extraction pipeline using a default sample text document.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Steps Explained\r\n",
    "\r\n",
    "### 1. Create a Default Sample Document\r\n",
    "```python\r\n",
    "sample_path = create_sample_document()  # Default is .txt\r\n",
    "```\r\n",
    "- Calls `create_sample_document()` without specifying a format.\r\n",
    "- By default, it creates a `.txt` document containing investment data.\r\n",
    "- The document includes field variations for realism.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. Display File Path\r\n",
    "```python\r\n",
    "print(f\"Created sample document: {sample_path}\")\r\n",
    "```\r\n",
    "- Confirms the file path of the generated document for traceability.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. Run the Full Pipeline\r\n",
    "```python\r\n",
    "result = main(sample_path)\r\n",
    "```\r\n",
    "- Calls the `main()` function, which runs all pipeline steps:\r\n",
    "  1. **Extraction**: Uses `DocumentExtractor` to read and parse the file.\r\n",
    "  2. **Processing**: Uses `DataProcessor` to clean and validate the data.\r\n",
    "  3. **Storage**: Uses `DataStorage` to save the results to a database and Excel file.\r\n",
    "  4. **Logging**: Displays statistics and identifies any issues.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. Display Completion Status\r\n",
    "```python\r\n",
    "if result:\r\n",
    "    print(\"... success\")\r\n",
    "else:\r\n",
    "    print(\"... issues\")\r\n",
    "```\r\n",
    "- If the `main()` function returns `True`, it means the pipeline completed successfully.\r\n",
    "- If `False`, an error occurred during one of the stages, and logs should be checked.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Summary\r\n",
    "This cell tests the full ETL pipeline from start to finish using a synthetic `.txt` document. It validates:\r\n",
    "- That each module works together as expected\r\n",
    "- That field mapping, formatting, and export features all function correctly\r\n",
    "- That error handling and logging are in place for production-level execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5af0cf8-442b-4d1f-9c5f-f4517efbaba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 17:02:17,995 - financial_data_extractor.main - INFO - Starting extraction process for sample_financial_document.txt\n",
      "2025-05-09 17:02:17,996 - financial_data_extractor.main - INFO - Extracting data from sample_financial_document.txt...\n",
      "2025-05-09 17:02:17,996 - financial_data_extractor.DocumentExtractor - INFO - Extracting text from sample_financial_document.txt (type: .txt)\n",
      "2025-05-09 17:02:17,998 - financial_data_extractor.DocumentExtractor - INFO - Successfully extracted 856 characters of text\n",
      "2025-05-09 17:02:17,998 - financial_data_extractor.DocumentExtractor - INFO - Starting data extraction\n",
      "2025-05-09 17:02:17,999 - financial_data_extractor.DocumentExtractor - INFO - Global as_of_date extracted: 03/31/2024\n",
      "2025-05-09 17:02:18,000 - financial_data_extractor.DocumentExtractor - INFO - No tabular data found, trying section-based extraction\n",
      "2025-05-09 17:02:18,000 - financial_data_extractor.DocumentExtractor - INFO - Identified 3 investment sections\n",
      "2025-05-09 17:02:18,001 - financial_data_extractor.DocumentExtractor - INFO - Extracted 3 valid records with data\n",
      "2025-05-09 17:02:18,002 - financial_data_extractor.main - INFO - Extracted 3 records.\n",
      "2025-05-09 17:02:18,002 - financial_data_extractor.main - INFO - Processing and formatting data...\n",
      "2025-05-09 17:02:18,002 - financial_data_extractor.DataProcessor - INFO - Formatting extracted data\n",
      "2025-05-09 17:02:18,003 - financial_data_extractor.DataProcessor - INFO - Formatted 3 records\n",
      "2025-05-09 17:02:18,003 - financial_data_extractor.DataProcessor - INFO - Calculating extraction statistics\n",
      "2025-05-09 17:02:18,004 - financial_data_extractor.DataProcessor - INFO - Statistics calculated: 100.00% mandatory fields extracted\n",
      "2025-05-09 17:02:18,004 - financial_data_extractor.main - INFO - Storing data...\n",
      "2025-05-09 17:02:18,004 - financial_data_extractor.DataStorage - INFO - Storing data in postgresql database\n",
      "2025-05-09 17:02:18,094 - financial_data_extractor.DataStorage - INFO - Successfully created financial_data_stats view\n",
      "2025-05-09 17:02:18,094 - financial_data_extractor.DataStorage - INFO - Successfully stored 3 records in database\n",
      "2025-05-09 17:02:18,095 - financial_data_extractor.main - INFO - Data successfully stored in postgresql database.\n",
      "2025-05-09 17:02:18,095 - financial_data_extractor.DataStorage - INFO - Storing data in Excel file: extracted_financial_data.xlsx\n",
      "2025-05-09 17:02:18,127 - financial_data_extractor.DataStorage - INFO - Successfully stored data in Excel file\n",
      "2025-05-09 17:02:18,128 - financial_data_extractor.main - INFO - Data successfully stored in Excel file: extracted_financial_data.xlsx\n",
      "2025-05-09 17:02:18,128 - financial_data_extractor.main - INFO - \n",
      "Extraction Statistics:\n",
      "2025-05-09 17:02:18,128 - financial_data_extractor.main - INFO - Total Records: 3\n",
      "2025-05-09 17:02:18,130 - financial_data_extractor.main - INFO - Mandatory Fields Extracted: 18\n",
      "2025-05-09 17:02:18,130 - financial_data_extractor.main - INFO - Mandatory Fields Percentage: 100.00%\n",
      "2025-05-09 17:02:18,131 - financial_data_extractor.main - INFO - Extraction Accuracy: 100.00%\n",
      "2025-05-09 17:02:18,131 - financial_data_extractor.main - INFO - Missing Fields: None\n",
      "2025-05-09 17:02:18,132 - financial_data_extractor.main - INFO - Inconsistent Data: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample document: sample_financial_document.txt\n",
      "\n",
      "Successfully completed the extraction, processing, and storage pipeline!\n"
     ]
    }
   ],
   "source": [
    "# Test the full pipeline with a sample document\n",
    "sample_path = create_sample_document()  # Default is .txt\n",
    "print(f\"Created sample document: {sample_path}\")\n",
    "\n",
    "# Run the main function\n",
    "result = main(sample_path)\n",
    "\n",
    "if result:\n",
    "    print(\"\\nSuccessfully completed the extraction, processing, and storage pipeline!\")\n",
    "else:\n",
    "    print(\"\\nPipeline execution encountered issues. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0664b4-e859-4b86-adef-95901014db58",
   "metadata": {},
   "source": [
    "# **Explanation for store in Excel and Database**\r\n",
    "\r\n",
    "This code cell demonstrates how the pipeline saves the processed data into both an Excel file and a PostgreSQL database, and verifies the results.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 1. Initialize DataStorage\r\n",
    "```python\r\n",
    "storage = DataStorage(processed_data, stats)\r\n",
    "```\r\n",
    "- Prepares the `DataStorage` object with the final data and statistics.\r\n",
    "- Uses it to export data to Excel and the database.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 2. Store in Excel\r\n",
    "```python\r\n",
    "excel_result = storage.store_in_excel()\r\n",
    "```\r\n",
    "- Calls the method to write:\r\n",
    "  - Extracted data → `Extracted Data` sheet\r\n",
    "  - Summary stats → `Statistics` sheet\r\n",
    "- Applies formatting to headers, highlights accuracy, and adjusts column widths.\r\n",
    "\r\n",
    "### If successful:\r\n",
    "```python\r\n",
    "df = pd.read_excel(...)\r\n",
    "display(df)\r\n",
    "```\r\n",
    "- Loads and displays the contents of the two sheets in Jupyter:\r\n",
    "  - `Extracted Data`: Record-by-record extracted information\r\n",
    "  - `Statistics`: Summary of field completeness, accuracy, and consistency\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 3. Drop Existing Database View\r\n",
    "```python\r\n",
    "connection.execute(text(\"DROP VIEW IF EXISTS financial_data_stats CASCADE;\"))\r\n",
    "```\r\n",
    "- Prepares the PostgreSQL database for a fresh write.\r\n",
    "- Ensures that previous versions of the stats view are removed to avoid conflicts.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 4. Store in Database and Verify\r\n",
    "```python\r\n",
    "db_result = storage.store_in_database()\r\n",
    "```\r\n",
    "- Stores data into the `financial_data` table using SQLAlchemy.\r\n",
    "- Automatically recreates a summary view `financial_data_stats` with field counts and distinct values.\r\n",
    "\r\n",
    "### Verification Queries:\r\n",
    "```python\r\n",
    "pd.read_sql(\"SELECT * FROM financial_data\", engine)\r\n",
    "pd.read_sql(\"SELECT * FROM financial_data_stats\", engine)\r\n",
    "```\r\n",
    "- Loads both the raw table and the stats view into pandas DataFrames.\r\n",
    "- Displays them within the Jupyter notebook.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Error Handling\r\n",
    "- Surrounds each operation with `try-except` to gracefully handle failures.\r\n",
    "- Prints meaningful error messages if any step fails.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Summary\r\n",
    "This cell ensures that the processed data is successfully saved, structured, and verifiable in both Excel and SQL environments. It completes the final step of the ETL pipeline by:\r\n",
    "- Writing to persistent storage\r\n",
    "- Formatting for human readability\r\n",
    "- Querying and verifying that the outputs match expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fefe9c4c-2fd3-49e3-b23b-36ef8d333d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 17:02:21,425 - financial_data_extractor.DataStorage - INFO - Storing data in Excel file: extracted_financial_data.xlsx\n",
      "2025-05-09 17:02:21,499 - financial_data_extractor.DataStorage - INFO - Successfully stored data in Excel file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data successfully stored in Excel file: extracted_financial_data.xlsx\n",
      "\n",
      "--- Excel Data Preview ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currency</th>\n",
       "      <th>sector</th>\n",
       "      <th>investment_in_prior</th>\n",
       "      <th>original_security_name</th>\n",
       "      <th>maturity_date</th>\n",
       "      <th>yield_percentage</th>\n",
       "      <th>investment_in_original</th>\n",
       "      <th>investment_in</th>\n",
       "      <th>as_of_date</th>\n",
       "      <th>risk_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USD</td>\n",
       "      <td>Technology</td>\n",
       "      <td>425800.0</td>\n",
       "      <td>Global Technology Fund Class A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.45</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>475250.00</td>\n",
       "      <td>03/31/2024</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USD</td>\n",
       "      <td>International</td>\n",
       "      <td>194325.0</td>\n",
       "      <td>Emerging Markets ETF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.85</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>180500.75</td>\n",
       "      <td>03/31/2024</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USD</td>\n",
       "      <td>Government</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>US Treasury Bond 2026</td>\n",
       "      <td>06/15/2026</td>\n",
       "      <td>4.25</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>250000.00</td>\n",
       "      <td>03/31/2024</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  currency         sector  investment_in_prior  \\\n",
       "0      USD     Technology             425800.0   \n",
       "1      USD  International             194325.0   \n",
       "2      USD     Government             250000.0   \n",
       "\n",
       "           original_security_name maturity_date  yield_percentage  \\\n",
       "0  Global Technology Fund Class A           NaN              2.45   \n",
       "1            Emerging Markets ETF           NaN              3.85   \n",
       "2           US Treasury Bond 2026    06/15/2026              4.25   \n",
       "\n",
       "   investment_in_original  investment_in  as_of_date risk_rating  \n",
       "0                400000.0      475250.00  03/31/2024    Moderate  \n",
       "1                200000.0      180500.75  03/31/2024        High  \n",
       "2                250000.0      250000.00  03/31/2024         Low  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Excel Statistics Preview ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Records Processed</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mandatory Fields Extracted</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mandatory Fields Percentage</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extraction Accuracy</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Missing Fields</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Inconsistent Data</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Field presence: currency</td>\n",
       "      <td>3 records (100.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Field presence: investment_in_prior</td>\n",
       "      <td>3 records (100.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Field presence: investment_in_original</td>\n",
       "      <td>3 records (100.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Field presence: investment_in</td>\n",
       "      <td>3 records (100.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Field presence: as_of_date</td>\n",
       "      <td>3 records (100.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Field presence: risk_rating</td>\n",
       "      <td>3 records (100.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Field presence: sector</td>\n",
       "      <td>3 records (100.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Field presence: original_security_name</td>\n",
       "      <td>3 records (100.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Field presence: maturity_date</td>\n",
       "      <td>3 records (100.00%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Field presence: yield_percentage</td>\n",
       "      <td>3 records (100.00%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Metric                Value\n",
       "0                  Total Records Processed                    3\n",
       "1               Mandatory Fields Extracted                   18\n",
       "2              Mandatory Fields Percentage              100.00%\n",
       "3                      Extraction Accuracy              100.00%\n",
       "4                           Missing Fields                  NaN\n",
       "5                        Inconsistent Data                  NaN\n",
       "6                 Field presence: currency  3 records (100.00%)\n",
       "7      Field presence: investment_in_prior  3 records (100.00%)\n",
       "8   Field presence: investment_in_original  3 records (100.00%)\n",
       "9            Field presence: investment_in  3 records (100.00%)\n",
       "10              Field presence: as_of_date  3 records (100.00%)\n",
       "11             Field presence: risk_rating  3 records (100.00%)\n",
       "12                  Field presence: sector  3 records (100.00%)\n",
       "13  Field presence: original_security_name  3 records (100.00%)\n",
       "14           Field presence: maturity_date  3 records (100.00%)\n",
       "15        Field presence: yield_percentage  3 records (100.00%)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 17:02:21,574 - financial_data_extractor.DataStorage - INFO - Storing data in postgresql database\n",
      "2025-05-09 17:02:21,623 - financial_data_extractor.DataStorage - INFO - Successfully created financial_data_stats view\n",
      "2025-05-09 17:02:21,624 - financial_data_extractor.DataStorage - INFO - Successfully stored 3 records in database\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully dropped the view (if it existed).\n",
      "Data successfully stored in postgresql database.\n",
      "\n",
      "--- Database Data Preview ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currency</th>\n",
       "      <th>sector</th>\n",
       "      <th>investment_in_prior</th>\n",
       "      <th>original_security_name</th>\n",
       "      <th>maturity_date</th>\n",
       "      <th>yield_percentage</th>\n",
       "      <th>investment_in_original</th>\n",
       "      <th>investment_in</th>\n",
       "      <th>as_of_date</th>\n",
       "      <th>risk_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USD</td>\n",
       "      <td>Technology</td>\n",
       "      <td>425800.00</td>\n",
       "      <td>Global Technology Fund Class A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2.45</td>\n",
       "      <td>400000.00</td>\n",
       "      <td>475250.00</td>\n",
       "      <td>03/31/2024</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USD</td>\n",
       "      <td>International</td>\n",
       "      <td>194325.00</td>\n",
       "      <td>Emerging Markets ETF</td>\n",
       "      <td>N/A</td>\n",
       "      <td>3.85</td>\n",
       "      <td>200000.00</td>\n",
       "      <td>180500.75</td>\n",
       "      <td>03/31/2024</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USD</td>\n",
       "      <td>Government</td>\n",
       "      <td>250000.00</td>\n",
       "      <td>US Treasury Bond 2026</td>\n",
       "      <td>06/15/2026</td>\n",
       "      <td>4.25</td>\n",
       "      <td>250000.00</td>\n",
       "      <td>250000.00</td>\n",
       "      <td>03/31/2024</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  currency         sector investment_in_prior          original_security_name  \\\n",
       "0      USD     Technology           425800.00  Global Technology Fund Class A   \n",
       "1      USD  International           194325.00            Emerging Markets ETF   \n",
       "2      USD     Government           250000.00           US Treasury Bond 2026   \n",
       "\n",
       "  maturity_date yield_percentage investment_in_original investment_in  \\\n",
       "0           N/A             2.45              400000.00     475250.00   \n",
       "1           N/A             3.85              200000.00     180500.75   \n",
       "2    06/15/2026             4.25              250000.00     250000.00   \n",
       "\n",
       "   as_of_date risk_rating  \n",
       "0  03/31/2024    Moderate  \n",
       "1  03/31/2024        High  \n",
       "2  03/31/2024         Low  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Database Stats View Preview ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_records</th>\n",
       "      <th>as_of_date_count</th>\n",
       "      <th>original_security_name_count</th>\n",
       "      <th>investment_in_original_count</th>\n",
       "      <th>investment_in_count</th>\n",
       "      <th>investment_in_prior_count</th>\n",
       "      <th>currency_count</th>\n",
       "      <th>currency_count_distinct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_records  as_of_date_count  original_security_name_count  \\\n",
       "0              3                 3                             3   \n",
       "\n",
       "   investment_in_original_count  investment_in_count  \\\n",
       "0                             3                    3   \n",
       "\n",
       "   investment_in_prior_count  currency_count  currency_count_distinct  \n",
       "0                          3               3                        1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the storage functionality with the sample data\n",
    "storage = DataStorage(processed_data, stats)\n",
    "\n",
    "# Store in Excel\n",
    "excel_result = storage.store_in_excel()\n",
    "if excel_result:\n",
    "    print(f\"\\nData successfully stored in Excel file: {CONFIG['output']['excel_file']}\")\n",
    "    # Read and display Excel file contents to verify\n",
    "    try:\n",
    "        df = pd.read_excel(CONFIG['output']['excel_file'], sheet_name='Extracted Data')\n",
    "        print(\"\\n--- Excel Data Preview ---\")\n",
    "        display(df)  # Jupyter will nicely format this\n",
    "        \n",
    "        stats_df = pd.read_excel(CONFIG['output']['excel_file'], sheet_name='Statistics')\n",
    "        print(\"\\n--- Excel Statistics Preview ---\")\n",
    "        display(stats_df)  # Jupyter will nicely format this\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel file: {e}\")\n",
    "else:\n",
    "    print(\"Failed to store data in Excel file.\")\n",
    "\n",
    "# Store in database\n",
    "try:\n",
    "    # Ensure view is dropped to avoid conflicts\n",
    "    connection_string = f\"postgresql://{CONFIG['database']['user']}:{CONFIG['database']['password']}@{CONFIG['database']['host']}:{CONFIG['database']['port']}/{CONFIG['database']['database']}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    from sqlalchemy import text\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"DROP VIEW IF EXISTS financial_data_stats CASCADE;\"))\n",
    "        connection.commit()\n",
    "    \n",
    "    print(\"Successfully dropped the view (if it existed).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error dropping view: {e}\")\n",
    "\n",
    "# Store data in database and verify\n",
    "try:\n",
    "    # Step 1: Store the data\n",
    "    db_result = storage.store_in_database()\n",
    "    if db_result:\n",
    "        print(f\"Data successfully stored in {CONFIG['database']['type']} database.\")\n",
    "        \n",
    "        connection_string = f\"postgresql://{CONFIG['database']['user']}:{CONFIG['database']['password']}@{CONFIG['database']['host']}:{CONFIG['database']['port']}/{CONFIG['database']['database']}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        \n",
    "        # Step 2: Verify the data\n",
    "        query = \"SELECT * FROM financial_data\"\n",
    "        db_data = pd.read_sql(query, engine)\n",
    "        \n",
    "        print(\"\\n--- Database Data Preview ---\")\n",
    "        display(db_data)\n",
    "        \n",
    "        # Step 3: Query the stats view\n",
    "        query = \"SELECT * FROM financial_data_stats\"\n",
    "        stats_data = pd.read_sql(query, engine)\n",
    "        \n",
    "        print(\"\\n--- Database Stats View Preview ---\")\n",
    "        display(stats_data)\n",
    "    else:\n",
    "        print(f\"Failed to store data in database.\")\n",
    "except Exception as e:\n",
    "    print(f\"Database operation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f10a58f-5615-4178-8a75-1f8b64e44d99",
   "metadata": {},
   "source": [
    "## **Result Summary**\n",
    "The pipeline successfully extracted and processed 3 investment records. All mandatory and additional fields were fully populated, resulting in 100% extraction accuracy and no missing or inconsistent data. The Excel output contained clean, standardized data with correct formatting, and the statistics sheet confirmed complete field presence. In the PostgreSQL database, both the financial_data table and the financial_data_stats view were created successfully. The view showed 3 total records, with full field counts and only one distinct currency (USD), confirming consistency across all records."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
